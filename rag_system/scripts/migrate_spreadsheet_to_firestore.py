#!/usr/bin/env python3
"""
Spreadsheetãƒ‡ãƒ¼ã‚¿ã‚’Firestoreã«ç§»æ¤

æ—¢å­˜ã®Vector DB Spreadsheetã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’Firestoreã«ç§»æ¤ã—ã¾ã™ã€‚
3072æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’PCAã§2048æ¬¡å…ƒã«åœ§ç¸®ã—ã¾ã™ã€‚

Usage:
    python scripts/migrate_spreadsheet_to_firestore.py --spreadsheet-id SPREADSHEET_ID --dry-run
    python scripts/migrate_spreadsheet_to_firestore.py --spreadsheet-id SPREADSHEET_ID --batch-size 100
    python scripts/migrate_spreadsheet_to_firestore.py --spreadsheet-id SPREADSHEET_ID --project fractal-ecosystem
"""

import os
import sys
import json
import time
import argparse
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import numpy as np
from sklearn.decomposition import PCA

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import google.auth
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
    from google.cloud import firestore
    from google.cloud.firestore_v1.vector import Vector
    from tqdm import tqdm
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    print("\nä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install google-auth google-auth-httplib2 google-api-python-client")
    print("pip install google-cloud-firestore tqdm scikit-learn numpy")
    sys.exit(1)

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / "backend" / ".env")

# è¨­å®š
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID", "fractal-ecosystem")
TARGET_DIMENSION = 2048  # Firestoreåˆ¶ç´„


class SpreadsheetToFirestoreMigrator:
    """Spreadsheetâ†’Firestoreç§»æ¤ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        spreadsheet_id: str,
        gcp_project_id: str,
        batch_size: int = 100,
        dry_run: bool = False
    ):
        self.spreadsheet_id = spreadsheet_id
        self.gcp_project_id = gcp_project_id
        self.batch_size = batch_size
        self.dry_run = dry_run

        # Google Sheets API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.sheets_service = self._init_sheets_service()

        # Firestore ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        if not dry_run:
            self.db = firestore.Client(project=gcp_project_id)
            self.collection = self.db.collection('knowledge_base')
        else:
            self.db = None
            self.collection = None

        # PCAãƒ¢ãƒ‡ãƒ«ï¼ˆåˆå›ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼‰
        self.pca: Optional[PCA] = None

        # çµ±è¨ˆ
        self.stats = {
            "total": 0,
            "success": 0,
            "skipped": 0,
            "errors": 0,
            "start_time": datetime.now(),
        }

    def _init_sheets_service(self):
        """Google Sheets API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            credentials, project = google.auth.default(
                scopes=[
                    'https://www.googleapis.com/auth/spreadsheets.readonly',
                    'https://www.googleapis.com/auth/drive.readonly',
                ]
            )
            return build('sheets', 'v4', credentials=credentials)
        except Exception as e:
            logger.error(f"Google Sheets APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def read_knowledge_base(self) -> List[Dict[str, Any]]:
        """KnowledgeBaseã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            logger.info("ğŸ“– KnowledgeBaseã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿é–‹å§‹...")

            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=self.spreadsheet_id,
                range='KnowledgeBase!A:Z'
            ).execute()

            values = result.get('values', [])
            if not values:
                logger.warning("KnowledgeBaseã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return []

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
            headers = values[0]
            data = []

            for row in values[1:]:
                # è¡ŒãŒçŸ­ã„å ´åˆã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
                row_data = row + [''] * (len(headers) - len(row))
                record = dict(zip(headers, row_data))
                data.append(record)

            logger.info(f"âœ… {len(data)}ä»¶ã®KnowledgeBaseãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return data

        except Exception as e:
            logger.error(f"KnowledgeBaseã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def read_embeddings(self) -> Dict[str, List[float]]:
        """Embeddingsã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆ3072æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼‰"""
        try:
            logger.info("ğŸ“– Embeddingsã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿é–‹å§‹...")

            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=self.spreadsheet_id,
                range='Embeddings!A:G'
            ).execute()

            values = result.get('values', [])
            if not values:
                logger.warning("Embeddingsã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return {}

            embeddings_dict = {}

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚¹ã‚­ãƒƒãƒ—
            for row in tqdm(values[1:], desc="Embeddingsèª­ã¿è¾¼ã¿"):
                if len(row) < 7:
                    logger.warning(f"è¡Œãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨: {row[:1]}")
                    continue

                kb_id = row[0]
                try:
                    # embedding_part1, part2, part3ã‚’çµåˆ
                    part1 = json.loads(row[3]) if row[3] else []
                    part2 = json.loads(row[4]) if row[4] else []
                    part3 = json.loads(row[5]) if row[5] else []

                    full_embedding = part1 + part2 + part3

                    if len(full_embedding) != 3072:
                        logger.warning(f"ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°ãŒä¸æ­£: {kb_id} (len={len(full_embedding)})")
                        continue

                    embeddings_dict[kb_id] = full_embedding

                except json.JSONDecodeError as e:
                    logger.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {kb_id} - {e}")
                    continue

            logger.info(f"âœ… {len(embeddings_dict)}ä»¶ã®Embeddingsã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return embeddings_dict

        except Exception as e:
            logger.error(f"Embeddingsã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def train_pca(self, embeddings: Dict[str, List[float]]):
        """PCAãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆ3072â†’2048æ¬¡å…ƒï¼‰"""
        logger.info("ğŸ”§ PCAãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")

        # å…¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¡Œåˆ—ã«å¤‰æ›
        vectors = list(embeddings.values())
        if not vectors:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

        X = np.array(vectors)
        logger.info(f"   å…¥åŠ›: {X.shape[0]}ä»¶ x {X.shape[1]}æ¬¡å…ƒ")

        # PCAã§2048æ¬¡å…ƒã«åœ§ç¸®
        self.pca = PCA(n_components=TARGET_DIMENSION, random_state=42)
        self.pca.fit(X)

        # æƒ…å ±ä¿æŒç‡ã‚’ç¢ºèª
        variance_ratio = self.pca.explained_variance_ratio_.sum()
        logger.info(f"âœ… PCAå­¦ç¿’å®Œäº†")
        logger.info(f"   å‡ºåŠ›: {TARGET_DIMENSION}æ¬¡å…ƒ")
        logger.info(f"   æƒ…å ±ä¿æŒç‡: {variance_ratio * 100:.2f}%")

    def compress_vector(self, vector: List[float]) -> List[float]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚’3072â†’2048æ¬¡å…ƒã«åœ§ç¸®"""
        if self.pca is None:
            raise ValueError("PCAãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        X = np.array([vector])
        compressed = self.pca.transform(X)[0]
        return compressed.tolist()

    def migrate_to_firestore(
        self,
        knowledge_base_data: List[Dict[str, Any]],
        embeddings_dict: Dict[str, List[float]]
    ):
        """Firestoreã®knowledgeBaseã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»æ¤"""

        if self.dry_run:
            logger.info("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰: Firestoreã¸ã®æ›¸ãè¾¼ã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

        logger.info(f"ğŸš€ Firestoreç§»æ¤é–‹å§‹...")
        logger.info(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {self.gcp_project_id}")
        logger.info(f"   ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: knowledge_base")
        logger.info(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")

        for i in tqdm(range(0, len(knowledge_base_data), self.batch_size), desc="Firestoreæ›¸ãè¾¼ã¿"):
            batch_data = knowledge_base_data[i:i + self.batch_size]

            if self.dry_run:
                # DRY RUN: çµ±è¨ˆã®ã¿æ›´æ–°
                for record in batch_data:
                    kb_id = record.get('id')
                    if kb_id in embeddings_dict:
                        self.stats['success'] += 1
                    else:
                        self.stats['skipped'] += 1
                    self.stats['total'] += 1
                continue

            # Firestoreãƒãƒƒãƒæ›¸ãè¾¼ã¿
            batch = self.db.batch()

            for record in batch_data:
                kb_id = record.get('id')
                if not kb_id:
                    logger.warning("IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    self.stats['skipped'] += 1
                    continue

                # ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
                embedding_3072 = embeddings_dict.get(kb_id)
                if not embedding_3072:
                    logger.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {kb_id}")
                    self.stats['skipped'] += 1
                    continue

                try:
                    # PCAåœ§ç¸®
                    embedding_2048 = self.compress_vector(embedding_3072)

                    # Firestoreå½¢å¼ã«å¤‰æ›
                    metadata_str = record.get('metadata', '{}')
                    metadata = json.loads(metadata_str) if metadata_str else {}

                    structured_data_str = record.get('structured_data', '{}')
                    structured_data = json.loads(structured_data_str) if structured_data_str else {}

                    # Firestoreãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
                    doc_ref = self.collection.document(kb_id)
                    doc_data = {
                        'id': kb_id,
                        'domain': record.get('domain', ''),
                        'source_type': record.get('source_type', ''),
                        'source_table': record.get('source_table', ''),
                        'source_id': record.get('source_id', ''),
                        'user_id': record.get('user_id', ''),
                        'title': record.get('title', ''),
                        'content': record.get('content', ''),
                        'structured_data': structured_data,
                        'metadata': metadata,
                        'embedding': Vector(embedding_2048),
                        'created_at': firestore.SERVER_TIMESTAMP,
                        'updated_at': firestore.SERVER_TIMESTAMP,
                    }

                    batch.set(doc_ref, doc_data)
                    self.stats['success'] += 1

                except Exception as e:
                    logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {kb_id} - {e}")
                    self.stats['errors'] += 1

                self.stats['total'] += 1

            # ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆ
            try:
                batch.commit()
            except Exception as e:
                logger.error(f"ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                self.stats['errors'] += len(batch_data)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
            time.sleep(0.1)

        logger.info("âœ… Firestoreç§»æ¤å®Œäº†")

    def run(self):
        """ç§»æ¤å®Ÿè¡Œ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ Spreadsheetâ†’Firestoreç§»æ¤é–‹å§‹")
            logger.info("=" * 60)
            logger.info(f"Spreadsheet ID: {self.spreadsheet_id}")
            logger.info(f"GCP Project: {self.gcp_project_id}")
            logger.info(f"Dry Run: {self.dry_run}")
            logger.info("")

            # 1. KnowledgeBaseèª­ã¿è¾¼ã¿
            knowledge_base_data = self.read_knowledge_base()
            if not knowledge_base_data:
                logger.error("KnowledgeBaseã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return

            # 2. Embeddingsèª­ã¿è¾¼ã¿
            embeddings_dict = self.read_embeddings()
            if not embeddings_dict:
                logger.error("Embeddingsã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return

            # 3. PCAå­¦ç¿’
            self.train_pca(embeddings_dict)

            # 4. Firestoreç§»æ¤
            self.migrate_to_firestore(knowledge_base_data, embeddings_dict)

            # çµ±è¨ˆè¡¨ç¤º
            duration = (datetime.now() - self.stats['start_time']).total_seconds()
            logger.info("")
            logger.info("=" * 60)
            logger.info("ğŸ“Š ç§»æ¤çµ±è¨ˆ")
            logger.info("=" * 60)
            logger.info(f"åˆè¨ˆä»¶æ•°: {self.stats['total']}")
            logger.info(f"æˆåŠŸ: {self.stats['success']}")
            logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {self.stats['skipped']}")
            logger.info(f"ã‚¨ãƒ©ãƒ¼: {self.stats['errors']}")
            logger.info(f"å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’")

            if self.stats['total'] > 0:
                success_rate = (self.stats['success'] / self.stats['total']) * 100
                logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")

        except Exception as e:
            logger.error(f"ç§»æ¤å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise


def main():
    parser = argparse.ArgumentParser(description='Spreadsheetãƒ‡ãƒ¼ã‚¿ã‚’Firestoreã«ç§»æ¤')
    parser.add_argument('--spreadsheet-id', required=True, help='Spreadsheet ID')
    parser.add_argument('--project', default=GCP_PROJECT_ID, help='GCP Project ID')
    parser.add_argument('--batch-size', type=int, default=100, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--dry-run', action='store_true', help='DRY RUNãƒ¢ãƒ¼ãƒ‰ï¼ˆæ›¸ãè¾¼ã¿ãªã—ï¼‰')
    args = parser.parse_args()

    migrator = SpreadsheetToFirestoreMigrator(
        spreadsheet_id=args.spreadsheet_id,
        gcp_project_id=args.project,
        batch_size=args.batch_size,
        dry_run=args.dry_run
    )

    migrator.run()


if __name__ == "__main__":
    main()
