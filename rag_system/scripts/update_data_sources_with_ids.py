#!/usr/bin/env python3
"""
extracted_spreadsheet_ids.jsonã‹ã‚‰data_sources.jsonã‚’æ›´æ–°

ãŸã ã—ã€GASå®Ÿè¡Œãƒ­ã‚°ã®IDã¯é™¤å¤–ã—ã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ä½¿ç”¨
"""

import json
from pathlib import Path

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
EXTRACTED_IDS_FILE = Path(__file__).parent / "extracted_spreadsheet_ids.json"
DATA_SOURCES_FILE = Path(__file__).parent / "data_sources.json"

# é™¤å¤–ã™ã‚‹Spreadsheet ID (GASå®Ÿè¡Œãƒ­ã‚°)
EXCLUDE_IDS = {
    "15Z_GT4-pDAnjDpd8vkX3B9FgYlQIQwdUF1QIEj7bVnE",  # GASå®Ÿè¡Œãƒ­ã‚°
    "16UHnMlSUlnUy-67gbwuvjeeU73AwDomqzJwGi6L4rVA",  # çµ±åˆå®Ÿè¡Œãƒ­ã‚°
    "16swPUizvdlyPxUjbDpVl9-VBDJZO91kX",  # å®Ÿè¡Œãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼
    "1roSp4WKubXVzZ6iWd6OY5lMU5OpvFsVNQHy11_Ym-wA",  # Vector DB
}

# æœ‰åŠ¹ã¨ç¢ºèªã•ã‚ŒãŸSpreadsheet IDï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åˆ¤å®šï¼‰
VALID_IDS = {
    "1EhLGOPKrxqMNl2b1_c0mA1M3w1tXiHN4PsnXWfWHSPw",  # è¨ªå•çœ‹è­·_é€šå¸¸è¨˜éŒ² (masterId)
    "1auRrqem2h3p7tcVs34sQci6eQeYflhcP-TAg1S4islg",  # å–¶æ¥­ãƒ¬ãƒãƒ¼ãƒˆ (SALES_ID)
    "1ctSjcAlu9VSloPT9S9hsTyTd7yCw5XvNtF7-URyBeKo",  # ååˆºå–ã‚Šè¾¼ã¿ (é–¢ä¿‚æ©Ÿé–¢_ç½®æ›SS)
}


def main():
    print("="*70)
    print("data_sources.jsonã‚’æ›´æ–°")
    print("="*70)

    # extracted_spreadsheet_ids.jsonã‚’èª­ã¿è¾¼ã¿
    with open(EXTRACTED_IDS_FILE, 'r', encoding='utf-8') as f:
        extracted_ids = json.load(f)

    # data_sources.jsonã‚’èª­ã¿è¾¼ã¿
    with open(DATA_SOURCES_FILE, 'r', encoding='utf-8') as f:
        data_sources = json.load(f)

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_file = DATA_SOURCES_FILE.parent / "data_sources.json.backup2"
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(data_sources, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_file}\n")

    # æ›´æ–°
    updated_count = 0
    skipped_count = 0

    for data_source_key, spreadsheet_id in extracted_ids.items():
        if not spreadsheet_id:
            print(f"  â­ï¸  {data_source_key:20s} : IDãªã—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        # é™¤å¤–IDã¯ã‚¹ã‚­ãƒƒãƒ—
        if spreadsheet_id in EXCLUDE_IDS:
            print(f"  â­ï¸  {data_source_key:20s} : {spreadsheet_id[:20]}... (GASå®Ÿè¡Œãƒ­ã‚°ã€ã‚¹ã‚­ãƒƒãƒ—)")
            skipped_count += 1
            continue

        # æœ‰åŠ¹ãªIDã®ã¿æ›´æ–°
        if spreadsheet_id in VALID_IDS:
            data_sources[data_source_key]['spreadsheet_id'] = spreadsheet_id
            print(f"  âœ… {data_source_key:20s} : {spreadsheet_id}")
            updated_count += 1
        else:
            print(f"  âš ï¸  {data_source_key:20s} : {spreadsheet_id[:20]}... (æœªæ¤œè¨¼ã€ã‚¹ã‚­ãƒƒãƒ—)")
            skipped_count += 1

    # ä¿å­˜
    with open(DATA_SOURCES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data_sources, f, ensure_ascii=False, indent=2)

    print("\n" + "="*70)
    print("ğŸ“Š æ›´æ–°çµæœ")
    print("="*70)
    print(f"âœ… æ›´æ–°: {updated_count}ä»¶")
    print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶")
    print(f"âŒ æœªè¨­å®š: {15 - updated_count - skipped_count}ä»¶")

    print("\nâš ï¸  é‡è¦: ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯Spreadsheet IDãŒæœªè¨­å®šã§ã™")
    print("AppSheetã‚¨ãƒ‡ã‚£ã‚¿ã‹ã‚‰ç›´æ¥ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š\n")

    unset_projects = []
    for data_source_key, config in data_sources.items():
        if config['spreadsheet_id'] == "ã€AppSheetã‚¢ãƒ—ãƒªã®Spreadsheet IDã‚’å…¥åŠ›ã€‘":
            unset_projects.append(f"  - {config['name']} ({data_source_key})")

    if unset_projects:
        for project in unset_projects:
            print(project)

    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. AppSheetã‚¨ãƒ‡ã‚£ã‚¿ã§æœªè¨­å®šã®Spreadsheet IDã‚’ç¢ºèª")
    print("2. data_sources.jsonã«æ‰‹å‹•ã§å…¥åŠ›")
    print("3. python scripts/vectorize_existing_data.py --list-sources ã§ç¢ºèª")
    print("\nå‚è€ƒ: scripts/HOW_TO_FIND_SPREADSHEET_IDS.md")


if __name__ == "__main__":
    main()
