#!/usr/bin/env python3
"""
Firestore â†’ MySQL ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Firestoreã®knowledge_baseã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¨embeddingãƒ‡ãƒ¼ã‚¿ã‚’Cloud SQL (MySQL)ã«ç§»è¡Œã—ã¾ã™ã€‚
"""

import asyncio
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from google.cloud import firestore
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError

from app.config import get_settings
from app.database import close_db, db_manager, init_db

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# è¨­å®š
settings = get_settings()


class FirestoreToMySQLMigrator:
    """Firestore â†’ MySQL ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self, dry_run: bool = False, batch_size: int = 100):
        """
        åˆæœŸåŒ–

        Args:
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ãªã—ï¼‰
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ï¼‰
        """
        self.dry_run = dry_run
        self.batch_size = batch_size

        # Firestore ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.firestore_db = firestore.Client(project=settings.gcp_project_id)
        self.collection_name = "knowledge_base"

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_documents": 0,
            "migrated_documents": 0,
            "migrated_embeddings": 0,
            "failed_documents": 0,
            "errors": [],
        }

        logger.info(f"ğŸ”§ Migrator initialized (dry_run={dry_run}, batch_size={batch_size})")

    def fetch_all_documents(self) -> List[Dict[str, Any]]:
        """
        Firestoreã‹ã‚‰å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—

        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸ“¥ Fetching documents from Firestore collection '{self.collection_name}'...")

        collection_ref = self.firestore_db.collection(self.collection_name)
        docs = collection_ref.stream()

        documents = []
        for doc in docs:
            data = doc.to_dict()
            data["_firestore_id"] = doc.id  # Firestore IDã‚’ä¿æŒ
            documents.append(data)

        self.stats["total_documents"] = len(documents)
        logger.info(f"âœ… Fetched {len(documents)} documents from Firestore")

        return documents

    def transform_document(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """
        Firestoreãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’MySQLå½¢å¼ã«å¤‰æ›

        Args:
            doc: Firestoreãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

        Returns:
            MySQLæŒ¿å…¥ç”¨ãƒ‡ãƒ¼ã‚¿
        """
        # IDã®å–å¾—ï¼ˆFirestore IDã¾ãŸã¯idãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
        doc_id = doc.get("id") or doc.get("_firestore_id")

        # æ—¥ä»˜ã®å¤‰æ›
        date_value = doc.get("date")
        if isinstance(date_value, datetime):
            date_str = date_value.strftime("%Y-%m-%d")
        elif isinstance(date_value, str):
            date_str = date_value
        else:
            date_str = None

        # JSON ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å¤‰æ›
        structured_data = doc.get("structured_data")
        if structured_data and not isinstance(structured_data, str):
            structured_data = json.dumps(structured_data, ensure_ascii=False)

        metadata = doc.get("metadata")
        if metadata and not isinstance(metadata, str):
            metadata = json.dumps(metadata, ensure_ascii=False)

        # knowledge_base ãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿
        kb_data = {
            "id": doc_id,
            "domain": doc.get("domain", "unknown"),
            "source_type": doc.get("source_type", "firestore"),
            "source_table": doc.get("source_table"),
            "source_id": doc.get("source_id"),
            "user_id": doc.get("user_id"),
            "user_name": doc.get("user_name"),
            "title": doc.get("title", ""),
            "content": doc.get("content", ""),
            "structured_data": structured_data,
            "metadata": metadata,
            "tags": doc.get("tags"),
            "date": date_str,
        }

        # embedding ãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿
        embedding_data = None
        if "embedding" in doc and doc["embedding"]:
            # Firestoreã®embeddingã¯ãƒªã‚¹ãƒˆå½¢å¼
            embedding_vector = doc["embedding"]

            # Vectorå‹ã®æ–‡å­—åˆ—è¡¨ç¾ã«å¤‰æ›ï¼ˆMySQL 9.0+ VECTORå‹ï¼‰
            # å½¢å¼: "[0.1, 0.2, 0.3, ...]"
            if isinstance(embedding_vector, list):
                embedding_str = json.dumps(embedding_vector)
            else:
                embedding_str = str(embedding_vector)

            embedding_data = {
                "kb_id": doc_id,
                "embedding": embedding_str,
                "embedding_model": doc.get("embedding_model", "gemini-embedding-001"),
            }

        return {"knowledge_base": kb_data, "embedding": embedding_data}

    async def insert_batch(
        self, knowledge_base_batch: List[Dict[str, Any]], embedding_batch: List[Dict[str, Any]]
    ) -> None:
        """
        ãƒãƒƒãƒæŒ¿å…¥

        Args:
            knowledge_base_batch: knowledge_baseæŒ¿å…¥ãƒ‡ãƒ¼ã‚¿
            embedding_batch: embeddingæŒ¿å…¥ãƒ‡ãƒ¼ã‚¿
        """
        if self.dry_run:
            logger.info(f"[DRY RUN] Would insert {len(knowledge_base_batch)} KB + {len(embedding_batch)} embeddings")
            return

        try:
            async with db_manager.get_session() as session:
                # knowledge_baseæŒ¿å…¥
                if knowledge_base_batch:
                    kb_insert_sql = text("""
                        INSERT INTO knowledge_base (
                            id, domain, source_type, source_table, source_id,
                            user_id, user_name, title, content,
                            structured_data, metadata, tags, date
                        ) VALUES (
                            :id, :domain, :source_type, :source_table, :source_id,
                            :user_id, :user_name, :title, :content,
                            :structured_data, :metadata, :tags, :date
                        )
                        ON DUPLICATE KEY UPDATE
                            title = VALUES(title),
                            content = VALUES(content),
                            updated_at = CURRENT_TIMESTAMP
                    """)

                    await session.execute(kb_insert_sql, knowledge_base_batch)
                    self.stats["migrated_documents"] += len(knowledge_base_batch)

                # embeddingæŒ¿å…¥
                if embedding_batch:
                    embedding_insert_sql = text("""
                        INSERT INTO embeddings (
                            kb_id, embedding, embedding_model
                        ) VALUES (
                            :kb_id, :embedding, :embedding_model
                        )
                        ON DUPLICATE KEY UPDATE
                            embedding = VALUES(embedding),
                            updated_at = CURRENT_TIMESTAMP
                    """)

                    await session.execute(embedding_insert_sql, embedding_batch)
                    self.stats["migrated_embeddings"] += len(embedding_batch)

                # ã‚³ãƒŸãƒƒãƒˆ
                await session.commit()

                logger.info(f"âœ… Inserted batch: {len(knowledge_base_batch)} KB + {len(embedding_batch)} embeddings")

        except SQLAlchemyError as e:
            logger.error(f"âŒ Batch insert failed: {e}", exc_info=True)
            self.stats["failed_documents"] += len(knowledge_base_batch)
            self.stats["errors"].append(str(e))
            raise

    async def migrate(self) -> Dict[str, Any]:
        """
        ç§»è¡Œå®Ÿè¡Œ

        Returns:
            çµ±è¨ˆæƒ…å ±
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("ğŸš€ Starting Firestore â†’ MySQL migration")
        logger.info("=" * 80)
        logger.info(f"Dry Run: {self.dry_run}")
        logger.info(f"Batch Size: {self.batch_size}")
        logger.info("=" * 80)

        try:
            # Firestore ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            documents = self.fetch_all_documents()

            if not documents:
                logger.warning("âš ï¸  No documents found in Firestore")
                return self.stats

            # ãƒãƒƒãƒå‡¦ç†
            kb_batch = []
            embedding_batch = []

            for i, doc in enumerate(documents, 1):
                try:
                    # å¤‰æ›
                    transformed = self.transform_document(doc)

                    # ãƒãƒƒãƒã«è¿½åŠ 
                    kb_batch.append(transformed["knowledge_base"])

                    if transformed["embedding"]:
                        embedding_batch.append(transformed["embedding"])

                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰æŒ¿å…¥
                    if len(kb_batch) >= self.batch_size:
                        await self.insert_batch(kb_batch, embedding_batch)
                        kb_batch = []
                        embedding_batch = []

                    # é€²æ—è¡¨ç¤º
                    if i % 100 == 0:
                        progress = (i / len(documents)) * 100
                        logger.info(f"ğŸ“Š Progress: {i}/{len(documents)} ({progress:.1f}%)")

                except Exception as e:
                    logger.error(f"âŒ Failed to process document {doc.get('_firestore_id')}: {e}")
                    self.stats["failed_documents"] += 1
                    self.stats["errors"].append(f"Doc {doc.get('_firestore_id')}: {str(e)}")

            # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’æŒ¿å…¥
            if kb_batch:
                await self.insert_batch(kb_batch, embedding_batch)

            # çµ±è¨ˆæƒ…å ±
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            self.stats["duration_seconds"] = duration
            self.stats["success_rate"] = (
                (self.stats["migrated_documents"] / self.stats["total_documents"]) * 100
                if self.stats["total_documents"] > 0
                else 0
            )

            logger.info("=" * 80)
            logger.info("âœ… Migration completed!")
            logger.info("=" * 80)
            logger.info(f"Total Documents: {self.stats['total_documents']}")
            logger.info(f"Migrated Documents: {self.stats['migrated_documents']}")
            logger.info(f"Migrated Embeddings: {self.stats['migrated_embeddings']}")
            logger.info(f"Failed Documents: {self.stats['failed_documents']}")
            logger.info(f"Success Rate: {self.stats['success_rate']:.2f}%")
            logger.info(f"Duration: {duration:.2f} seconds")
            logger.info("=" * 80)

            return self.stats

        except Exception as e:
            logger.error(f"âŒ Migration failed: {e}", exc_info=True)
            self.stats["errors"].append(f"Fatal error: {str(e)}")
            raise


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(description="Firestore â†’ MySQL ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ")
    parser.add_argument("--dry-run", action="store_true", help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®æ›¸ãè¾¼ã¿ãªã—ï¼‰")
    parser.add_argument("--batch-size", type=int, default=100, help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰")
    parser.add_argument("--output", type=str, help="çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›å…ˆJSONãƒ•ã‚¡ã‚¤ãƒ«")

    args = parser.parse_args()

    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        logger.info("ğŸ”„ Initializing database connection...")
        await init_db()
        logger.info("âœ… Database connection initialized")

        # ç§»è¡Œå®Ÿè¡Œ
        migrator = FirestoreToMySQLMigrator(dry_run=args.dry_run, batch_size=args.batch_size)
        stats = await migrator.migrate()

        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
        if args.output:
            output_path = Path(args.output)
            output_path.write_text(json.dumps(stats, indent=2, ensure_ascii=False))
            logger.info(f"ğŸ“„ Stats written to {output_path}")

    except Exception as e:
        logger.error(f"âŒ Migration failed: {e}", exc_info=True)
        sys.exit(1)
    finally:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º
        await close_db()
        logger.info("ğŸ”’ Database connection closed")


if __name__ == "__main__":
    asyncio.run(main())
