# LangChain + LangGraph çµ±åˆå®Ÿè£…ã‚¬ã‚¤ãƒ‰

> **Document Version:** 1.0
> **Last Updated:** 2025å¹´10æœˆ9æ—¥
> **Decision:** ADR-007ã§æ¡ç”¨æ±ºå®š
> **Status:** âœ… ç¢ºå®š

## ç›®æ¬¡

1. [æ¦‚è¦](#1-æ¦‚è¦)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ](#2-ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ)
3. [LangChainå®Ÿè£…ï¼ˆRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)](#3-langchainå®Ÿè£…ragãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
4. [LangGraphå®Ÿè£…ï¼ˆãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)](#4-langgraphå®Ÿè£…ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
5. [Anthropic Research-First Workflowçµ±åˆ](#5-anthropic-research-first-workflowçµ±åˆ)
6. [æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤](#6-æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤)
7. [ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°](#7-ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°)

---

## 1. æ¦‚è¦

### 1.1. æ¡ç”¨æ±ºå®šç†ç”±

| é …ç›® | å†…å®¹ |
|------|------|
| **æ±ºå®šæ—¥** | 2025å¹´10æœˆ9æ—¥ |
| **æ±ºå®šå†…å®¹** | LangChain + LangGraph ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ |
| **ä¸»ãªç†ç”±** | æœ¬ç•ªå®Ÿç¸¾ï¼ˆ400ç¤¾+ï¼‰ã€Vertex AIçµ±åˆã€æˆç†Ÿã—ãŸã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  |
| **ä»£æ›¿æ¡ˆ** | Google ADKï¼ˆæ—©æœŸæ®µéšï¼‰ã€AutoGenï¼ˆè¤‡é›‘æ€§ï¼‰ |

### 1.2. å½¹å‰²åˆ†æ‹…

```mermaid
graph LR
    A[ğŸ” RAG Pipeline] -->|LangChain| B[ç·šå½¢å‡¦ç†<br/>Retrieval<br/>Embedding<br/>Hybrid Search]
    C[ğŸ¤– Multi-Agent] -->|LangGraph| D[ä¸¦åˆ—å‡¦ç†<br/>Orchestration<br/>State Management<br/>Supervisor Pattern]

    B --> E[Vertex AIçµ±åˆ]
    D --> E

    E --> F[FastAPI Backend]

    classDef langchainStyle fill:#00acc1,stroke:#26c6da,stroke-width:2px,color:#fff
    classDef langgraphStyle fill:#7e57c2,stroke:#9575cd,stroke-width:2px,color:#fff
    classDef integrationStyle fill:#66bb6a,stroke:#81c784,stroke-width:2px,color:#fff

    class A,B langchainStyle
    class C,D langgraphStyle
    class E,F integrationStyle
```

**ç›¸è£œçš„ãªé–¢ä¿‚:**

- **LangChain:** RAGã®ç·šå½¢å‡¦ç†ï¼ˆRetrieval â†’ Ranking â†’ Generationï¼‰
- **LangGraph:** ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹åˆ¶å¾¡ï¼ˆä¸¦åˆ—å®Ÿè¡Œ â†’ çµ±åˆï¼‰

---

## 2. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### 2.1. 7ãƒ•ã‚§ãƒ¼ã‚ºRAG + ãƒãƒ«ãƒLLMã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆ

```mermaid
graph TD
    A[ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•] -->|LangChain| B[Phase 1-5<br/>RAG Pipeline]

    B --> C[Phase 6: Confidence Check]

    C -->|ä¿¡é ¼åº¦ > 0.8| D[Phase 7<br/>Multi-LLM Ensemble]
    C -->|ä¿¡é ¼åº¦ < 0.8| E[Agentic RAG<br/>Query Transformation]

    E -.->|å†æ¤œç´¢| B

    D -->|LangGraph| F{Supervisor}

    F -->|ä¸¦åˆ—å®Ÿè¡Œ| G1[Gemini Agent]
    F -->|ä¸¦åˆ—å®Ÿè¡Œ| G2[DeepSeek Agent]
    F -->|ä¸¦åˆ—å®Ÿè¡Œ| G3[Groq Agent]

    G1 & G2 & G3 --> H[Synthesizer Node]

    H --> I[âœ¨ æœ€çµ‚å›ç­”]

    classDef userStyle fill:#00acc1,stroke:#26c6da,stroke-width:2px,color:#fff
    classDef langchainStyle fill:#7e57c2,stroke:#9575cd,stroke-width:2px,color:#fff
    classDef langgraphStyle fill:#f57c00,stroke:#ff9800,stroke-width:2px,color:#fff
    classDef resultStyle fill:#66bb6a,stroke:#81c784,stroke-width:2px,color:#fff

    class A userStyle
    class B,C,E langchainStyle
    class D,F,G1,G2,G3,H langgraphStyle
    class I resultStyle
```

### 2.2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```text
app/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # LangChain RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”‚   â”‚   â”œâ”€â”€ embeddings.py         # Vertex AI Embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # Vector Searchçµ±åˆ
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py      # Vector + BM25
â”‚   â”‚   â”œâ”€â”€ reranker.py           # Gemini Re-ranking
â”‚   â”‚   â””â”€â”€ chunker.py            # Semantic Chunking
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph StateGraph
â”‚   â”‚   â”œâ”€â”€ nodes.py              # Agent Nodeså®Ÿè£…
â”‚   â”‚   â”œâ”€â”€ state.py              # Shared Stateå®šç¾©
â”‚   â”‚   â”œâ”€â”€ supervisor.py         # Supervisor Pattern
â”‚   â”‚   â””â”€â”€ synthesizer.py        # Flash-Liteçµ±åˆ
â”‚   â”‚
â”‚   â””â”€â”€ vertex_ai/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ client.py             # Vertex AI Client
â”‚       â””â”€â”€ models.py             # Model Configuration
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ v1/
â”‚       â””â”€â”€ endpoints/
â”‚           â””â”€â”€ chat.py           # Chat API (LangGraphçµ±åˆ)
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_pipeline.py
    â””â”€â”€ test_agents.py
```

---

## 3. LangChainå®Ÿè£…ï¼ˆRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰

### 3.1. Vertex AI Embeddingsçµ±åˆ

```python
# app/services/rag/embeddings.py

from langchain_google_vertexai import VertexAIEmbeddings
from typing import List
import logging

logger = logging.getLogger(__name__)

class FractalLinkEmbeddings:
    """fractal-linkç”¨Vertex AI Embeddings (gemini-embedding-001)"""

    def __init__(
        self,
        project_id: str,
        location: str = "asia-northeast1",
        model_name: str = "gemini-embedding-001"
    ):
        self.embeddings = VertexAIEmbeddings(
            model_name=model_name,
            project=project_id,
            location=location
        )
        logger.info(f"Initialized Vertex AI Embeddings: {model_name}")

    def embed_query(self, text: str) -> List[float]:
        """ã‚¯ã‚¨ãƒªç”¨Embeddingï¼ˆtask_type=RETRIEVAL_QUERYï¼‰"""
        return self.embeddings.embed_query(
            text,
            task_type="RETRIEVAL_QUERY",
            output_dimensionality=3072  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
        )

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨Embeddingï¼ˆtask_type=RETRIEVAL_DOCUMENTï¼‰"""
        return self.embeddings.embed_documents(
            texts,
            task_type="RETRIEVAL_DOCUMENT",
            output_dimensionality=3072
        )
```

### 3.2. Vector Searchçµ±åˆ

```python
# app/services/rag/vector_store.py

from langchain_google_vertexai import VectorSearchVectorStore
from langchain_core.documents import Document
from typing import List
import logging

logger = logging.getLogger(__name__)

class FractalLinkVectorStore:
    """Vertex AI Vector Searchçµ±åˆ"""

    def __init__(
        self,
        project_id: str,
        location: str,
        index_id: str,
        endpoint_id: str,
        embeddings: FractalLinkEmbeddings
    ):
        self.vector_store = VectorSearchVectorStore(
            project_id=project_id,
            region=location,
            index_id=index_id,
            endpoint_id=endpoint_id,
            embedding=embeddings.embeddings,
            gcs_bucket_name=f"{project_id}-fractal-link-docs"
        )
        logger.info(f"Initialized Vector Store: {index_id}")

    def similarity_search(
        self,
        query: str,
        k: int = 20,
        filter: dict = None
    ) -> List[Document]:
        """Vectoré¡ä¼¼åº¦æ¤œç´¢ï¼ˆTop Kï¼‰"""
        results = self.vector_store.similarity_search(
            query=query,
            k=k,
            filter=filter
        )
        logger.info(f"Vector search returned {len(results)} results")
        return results

    def add_documents(self, documents: List[Document]):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ï¼ˆIndexingï¼‰"""
        self.vector_store.add_documents(documents)
        logger.info(f"Added {len(documents)} documents to vector store")
```

### 3.3. Hybrid Searchå®Ÿè£…

```python
# app/services/rag/hybrid_search.py

from typing import List
from langchain_core.documents import Document
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)

class HybridSearchRetriever:
    """Vector Search + BM25 Hybrid Search"""

    def __init__(
        self,
        vector_store: FractalLinkVectorStore,
        bm25_scorer: 'JapaneseBM25Scorer',
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3,
        k: int = 60  # RRF constant
    ):
        self.vector_store = vector_store
        self.bm25_scorer = bm25_scorer
        self.vector_weight = vector_weight
        self.bm25_weight = bm25_weight
        self.k = k

    def retrieve(self, query: str, top_k: int = 20) -> List[Document]:
        """Reciprocal Rank Fusion (RRF)ã§çµ±åˆæ¤œç´¢"""

        # 1. Vector Search
        vector_results = self.vector_store.similarity_search(
            query=query,
            k=top_k * 2  # ä½™è£•ã‚’æŒã£ã¦å–å¾—
        )

        # 2. BM25 Search
        bm25_results = self.bm25_scorer.search(
            query=query,
            top_k=top_k * 2
        )

        # 3. RRFçµ±åˆ
        scores = defaultdict(float)

        for rank, doc in enumerate(vector_results):
            doc_id = doc.metadata.get("chunk_id")
            scores[doc_id] += self.vector_weight / (self.k + rank)

        for rank, doc in enumerate(bm25_results):
            doc_id = doc.metadata.get("chunk_id")
            scores[doc_id] += self.bm25_weight / (self.k + rank)

        # 4. ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        # 5. Top Kå–å¾—
        final_docs = []
        doc_map = {d.metadata["chunk_id"]: d for d in vector_results + bm25_results}

        for doc_id, score in sorted_ids[:top_k]:
            doc = doc_map.get(doc_id)
            if doc:
                doc.metadata["hybrid_score"] = score
                final_docs.append(doc)

        logger.info(f"Hybrid search returned {len(final_docs)} documents")
        return final_docs
```

### 3.4. Semantic Chunking

```python
# app/services/rag/chunker.py

from langchain_experimental.text_splitter import SemanticChunker
from langchain_google_vertexai import VertexAIEmbeddings
from langchain_core.documents import Document
from typing import List
import logging

logger = logging.getLogger(__name__)

class FractalLinkChunker:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ï¼ˆLangChainå…¬å¼æ¨å¥¨ï¼‰"""

    def __init__(self, embeddings: FractalLinkEmbeddings):
        self.chunker = SemanticChunker(
            embeddings=embeddings.embeddings,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95,  # 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
            number_of_chunks=None  # è‡ªå‹•æ±ºå®š
        )
        logger.info("Initialized Semantic Chunker")

    def chunk_document(
        self,
        text: str,
        metadata: dict = None
    ) -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å¢ƒç•Œã§åˆ†å‰²"""

        chunks = self.chunker.create_documents(
            texts=[text],
            metadatas=[metadata] if metadata else None
        )

        # ãƒãƒ£ãƒ³ã‚¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_size": len(chunk.page_content)
            })

        logger.info(f"Created {len(chunks)} semantic chunks")
        return chunks
```

### 3.5. RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ

```python
# app/services/rag/pipeline.py

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_google_vertexai import ChatVertexAI
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class RAGPipeline:
    """7ãƒ•ã‚§ãƒ¼ã‚ºRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆLangChain LCELï¼‰"""

    def __init__(
        self,
        hybrid_retriever: HybridSearchRetriever,
        reranker: 'GeminiReranker',
        llm: ChatVertexAI
    ):
        self.retriever = hybrid_retriever
        self.reranker = reranker
        self.llm = llm

        # LCEL Chainæ§‹ç¯‰
        self.chain = self._build_chain()
        logger.info("Initialized RAG Pipeline")

    def _build_chain(self):
        """LangChain Expression Language (LCEL) Chain"""

        # Phase 1-2: Hybrid Retrieval
        retrieval = RunnablePassthrough.assign(
            documents=lambda x: self.retriever.retrieve(
                query=x["query"],
                top_k=20
            )
        )

        # Phase 3: Re-ranking
        reranking = RunnablePassthrough.assign(
            ranked_docs=lambda x: self.reranker.rerank(
                query=x["query"],
                documents=x["documents"],
                top_k=5
            )
        )

        # Phase 4: Context Formatting
        context_format = RunnablePassthrough.assign(
            context=lambda x: self._format_context(x["ranked_docs"])
        )

        # Phase 5: Confidence Check (LangGraphã§å‡¦ç†)
        # Phase 6-7: Multi-LLM Ensemble (LangGraphã§å‡¦ç†)

        # Chainé€£çµ
        chain = retrieval | reranking | context_format

        return chain

    def _format_context(self, documents: List[Document]) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢"""
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(
                f"ã€æ–‡æ›¸{i}ã€‘\n"
                f"å‡ºå…¸: {doc.metadata.get('filename', 'Unknown')}\n"
                f"å†…å®¹:\n{doc.page_content}\n"
            )
        return "\n".join(context_parts)

    def invoke(self, query: str) -> Dict[str, Any]:
        """RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        result = self.chain.invoke({"query": query})
        logger.info("RAG pipeline completed")
        return result
```

---

## 4. LangGraphå®Ÿè£…ï¼ˆãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰

### 4.1. Stateå®šç¾©ï¼ˆShared Memory + Private Scratchpadï¼‰

```python
# app/services/agents/state.py

from typing import TypedDict, List, Dict, Annotated
import operator
from langchain_core.messages import BaseMessage

class SharedState(TypedDict):
    """å…±æœ‰çŠ¶æ…‹ï¼ˆå…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‚ç…§å¯èƒ½ï¼‰"""
    messages: Annotated[List[BaseMessage], operator.add]
    query: str
    context: str
    final_answer: str
    confidence: float
    iteration: int
    max_iterations: int

class AgentScratchpad(TypedDict):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå›ºæœ‰ã®ä½œæ¥­é ˜åŸŸï¼ˆéå…±æœ‰ï¼‰"""
    agent_name: str
    thinking: List[str]
    intermediate_answer: str
    sources: List[str]
```

### 4.2. Agent Nodeså®Ÿè£…

```python
# app/services/agents/nodes.py

from typing import Dict, Any
from langchain_google_vertexai import ChatVertexAI
from langchain_core.messages import HumanMessage, AIMessage
import asyncio
import logging

logger = logging.getLogger(__name__)

class MultiLLMAgentNodes:
    """3ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«Agent Nodes"""

    def __init__(self):
        # Gemini 2.5 Flash-Lite
        self.gemini_lite = ChatVertexAI(
            model_name="gemini-2.5-flash-lite",
            temperature=0.3,
            max_output_tokens=500
        )

        # DeepSeek V3.2 (OpenAIäº’æ›)
        from langchain_openai import ChatOpenAI
        self.deepseek = ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com",
            temperature=0.3,
            max_tokens=500
        )

        # Groq Llama 3.1 8B
        self.groq_llama = ChatOpenAI(
            model="llama-3.1-8b-instant",
            api_key=os.getenv("GROQ_API_KEY"),
            base_url="https://api.groq.com/openai/v1",
            temperature=0.3,
            max_tokens=500
        )

        logger.info("Initialized Multi-LLM Agent Nodes")

    async def gemini_agent_node(self, state: SharedState) -> Dict:
        """Gemini Flash-Lite Agent Node"""
        logger.info("Gemini agent processing...")

        prompt = self._build_agent_prompt(
            query=state["query"],
            context=state["context"]
        )

        response = await self.gemini_lite.ainvoke([HumanMessage(content=prompt)])

        return {
            "messages": [AIMessage(content=response.content, name="gemini")],
            "gemini_answer": response.content
        }

    async def deepseek_agent_node(self, state: SharedState) -> Dict:
        """DeepSeek V3.2 Agent Node"""
        logger.info("DeepSeek agent processing...")

        prompt = self._build_agent_prompt(
            query=state["query"],
            context=state["context"]
        )

        response = await self.deepseek.ainvoke([HumanMessage(content=prompt)])

        return {
            "messages": [AIMessage(content=response.content, name="deepseek")],
            "deepseek_answer": response.content
        }

    async def groq_agent_node(self, state: SharedState) -> Dict:
        """Groq Llama 3.1 8B Agent Node"""
        logger.info("Groq agent processing...")

        prompt = self._build_agent_prompt(
            query=state["query"],
            context=state["context"]
        )

        response = await self.groq_llama.ainvoke([HumanMessage(content=prompt)])

        return {
            "messages": [AIMessage(content=response.content, name="groq")],
            "groq_answer": response.content
        }

    def _build_agent_prompt(self, query: str, context: str) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        return f"""ã‚ãªãŸã¯è¨ªå•çœ‹è­·ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å‚ç…§æ–‡æ›¸ã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{query}

ã€å‚ç…§æ–‡æ›¸ã€‘
{context}

ã€æŒ‡ç¤ºã€‘
- å‚ç…§æ–‡æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„
- æ ¹æ‹ ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„
- æ¨æ¸¬ã¯é¿ã‘ã€äº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
- 500æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„

ã€å›ç­”ã€‘"""
```

### 4.3. Synthesizer Nodeå®Ÿè£…

```python
# app/services/agents/synthesizer.py

from typing import Dict
from langchain_google_vertexai import ChatVertexAI
from langchain_core.messages import HumanMessage
import logging

logger = logging.getLogger(__name__)

class SynthesizerNode:
    """Flash-Liteçµ±åˆãƒãƒ¼ãƒ‰"""

    def __init__(self):
        self.synthesizer = ChatVertexAI(
            model_name="gemini-2.5-flash-lite",
            temperature=0.2,  # ä¸€è²«æ€§é‡è¦–
            max_output_tokens=800
        )
        logger.info("Initialized Synthesizer Node")

    async def synthesize_node(self, state: Dict) -> Dict:
        """3ãƒ¢ãƒ‡ãƒ«å›ç­”ã‚’çµ±åˆ"""
        logger.info("Synthesizing multi-agent responses...")

        prompt = f"""ä»¥ä¸‹ã®3ã¤ã®AIå›ç­”ã‚’çµ±åˆã—ã€æœ€ã‚‚æ­£ç¢ºã§å®Œå…¨ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{state["query"]}

ã€Gemini Flash-Liteå›ç­”ã€‘
{state.get("gemini_answer", "[å¿œç­”ãªã—]")}

ã€DeepSeek V3.2å›ç­”ã€‘
{state.get("deepseek_answer", "[å¿œç­”ãªã—]")}

ã€Groq Llamaå›ç­”ã€‘
{state.get("groq_answer", "[å¿œç­”ãªã—]")}

ã€çµ±åˆæŒ‡ç¤ºã€‘
1. å…±é€šç‚¹: 3ãƒ¢ãƒ‡ãƒ«ãŒä¸€è‡´ã™ã‚‹æƒ…å ±ã¯ä¿¡é ¼æ€§ãŒé«˜ã„ãŸã‚å¿…ãšå«ã‚ã‚‹
2. çŸ›ç›¾ç‚¹: æœ€ã‚‚åŒ»å­¦çš„æ ¹æ‹ ãŒå¼·ã„æƒ…å ±ã‚’æ¡ç”¨ã—ã€ç†ç”±ã‚’æ˜è¨˜
3. è£œå®Œ: å„ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®æœ‰ç›Šãªæƒ…å ±ã‚’çµ±åˆ
4. å‡ºå…¸: å‚ç…§ã—ãŸæ–‡æ›¸ã‚’æ˜è¨˜

ã€å‡ºåŠ›å½¢å¼ã€‘
- ç°¡æ½”ã§å®Ÿç”¨çš„ãªå›ç­”ï¼ˆ800æ–‡å­—ä»¥å†…ï¼‰
- æ ¹æ‹ ã¨å‡ºå…¸ã‚’æ˜ç¤º
- åŒ»ç™‚ã‚¹ã‚¿ãƒƒãƒ•ãŒå®Ÿå‹™ã§ä½¿ç”¨ã§ãã‚‹å†…å®¹

ã€çµ±åˆå›ç­”ã€‘"""

        response = await self.synthesizer.ainvoke([HumanMessage(content=prompt)])

        return {
            "final_answer": response.content,
            "confidence": 0.95  # 3ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚‹é«˜ä¿¡é ¼åº¦
        }
```

### 4.4. LangGraph StateGraphæ§‹ç¯‰

```python
# app/services/agents/graph.py

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import Dict
import logging

logger = logging.getLogger(__name__)

class MultiAgentRAGGraph:
    """LangGraph: Multi-Agent RAG Orchestration"""

    def __init__(
        self,
        agent_nodes: MultiLLMAgentNodes,
        synthesizer: SynthesizerNode
    ):
        self.agent_nodes = agent_nodes
        self.synthesizer = synthesizer

        # StateGraphæ§‹ç¯‰
        self.graph = self._build_graph()

        # Memory Saverï¼ˆä¼šè©±å±¥æ­´ä¿æŒï¼‰
        self.checkpointer = MemorySaver()

        # Compile
        self.app = self.graph.compile(checkpointer=self.checkpointer)

        logger.info("Initialized Multi-Agent RAG Graph")

    def _build_graph(self) -> StateGraph:
        """StateGraphæ§‹ç¯‰"""
        workflow = StateGraph(SharedState)

        # Nodesè¿½åŠ 
        workflow.add_node("gemini_agent", self.agent_nodes.gemini_agent_node)
        workflow.add_node("deepseek_agent", self.agent_nodes.deepseek_agent_node)
        workflow.add_node("groq_agent", self.agent_nodes.groq_agent_node)
        workflow.add_node("synthesizer", self.synthesizer.synthesize_node)

        # Entry point
        workflow.set_entry_point("gemini_agent")

        # Edgesï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
        workflow.add_edge("gemini_agent", "deepseek_agent")
        workflow.add_edge("gemini_agent", "groq_agent")

        # çµ±åˆãƒãƒ¼ãƒ‰ã¸
        workflow.add_edge("deepseek_agent", "synthesizer")
        workflow.add_edge("groq_agent", "synthesizer")

        # End
        workflow.add_edge("synthesizer", END)

        return workflow

    async def invoke(self, query: str, context: str) -> Dict:
        """ã‚°ãƒ©ãƒ•å®Ÿè¡Œ"""
        initial_state = {
            "messages": [],
            "query": query,
            "context": context,
            "final_answer": "",
            "confidence": 0.0,
            "iteration": 0,
            "max_iterations": 3
        }

        result = await self.app.ainvoke(initial_state)

        logger.info("Multi-agent graph execution completed")
        return result
```

---

## 5. Anthropic Research-First Workflowçµ±åˆ

### 5.1. Research Agent Pattern

```python
# app/services/agents/research_agent.py

from typing import Dict, List
from langgraph.graph import StateGraph, END
import logging

logger = logging.getLogger(__name__)

class ResearchFirstAgent:
    """Anthropic Research-First Workflow Pattern"""

    def __init__(self, rag_pipeline: RAGPipeline):
        self.rag_pipeline = rag_pipeline
        self.graph = self._build_research_graph()
        self.app = self.graph.compile()

    def _build_research_graph(self) -> StateGraph:
        """Research-First Graphæ§‹ç¯‰"""
        workflow = StateGraph(SharedState)

        # Research Phase Nodes
        workflow.add_node("analyze_query", self._analyze_query_node)
        workflow.add_node("gather_info", self._gather_info_node)
        workflow.add_node("verify_info", self._verify_info_node)
        workflow.add_node("generate_answer", self._generate_answer_node)

        # Entry
        workflow.set_entry_point("analyze_query")

        # Edges with conditional routing
        workflow.add_conditional_edges(
            "analyze_query",
            self._should_gather_more_info,
            {
                "gather": "gather_info",
                "generate": "generate_answer"
            }
        )

        workflow.add_edge("gather_info", "verify_info")
        workflow.add_conditional_edges(
            "verify_info",
            self._is_info_sufficient,
            {
                "sufficient": "generate_answer",
                "insufficient": "gather_info"
            }
        )

        workflow.add_edge("generate_answer", END)

        return workflow

    async def _analyze_query_node(self, state: Dict) -> Dict:
        """Step 1: ã‚¯ã‚¨ãƒªåˆ†æ"""
        logger.info("Analyzing query complexity...")

        # ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã‚’è©•ä¾¡
        query_complexity = self._assess_complexity(state["query"])

        return {
            **state,
            "complexity": query_complexity
        }

    async def _gather_info_node(self, state: Dict) -> Dict:
        """Step 2: æƒ…å ±åé›†ï¼ˆRAG Pipelineä½¿ç”¨ï¼‰"""
        logger.info("Gathering information from RAG...")

        rag_result = self.rag_pipeline.invoke(state["query"])

        return {
            **state,
            "context": rag_result["context"],
            "sources": rag_result["ranked_docs"]
        }

    async def _verify_info_node(self, state: Dict) -> Dict:
        """Step 3: æƒ…å ±æ¤œè¨¼"""
        logger.info("Verifying information sufficiency...")

        # ä¿¡é ¼åº¦è©•ä¾¡
        confidence = self._calculate_confidence(
            query=state["query"],
            context=state["context"]
        )

        return {
            **state,
            "confidence": confidence,
            "iteration": state["iteration"] + 1
        }

    def _should_gather_more_info(self, state: Dict) -> str:
        """æ¡ä»¶åˆ†å²: æƒ…å ±åé›†ãŒå¿…è¦ã‹"""
        if state.get("complexity", "medium") == "high":
            return "gather"
        return "generate"

    def _is_info_sufficient(self, state: Dict) -> str:
        """æ¡ä»¶åˆ†å²: æƒ…å ±ãŒååˆ†ã‹"""
        if state["confidence"] > 0.8 or state["iteration"] >= state["max_iterations"]:
            return "sufficient"
        return "insufficient"
```

---

## 6. æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

### 6.1. FastAPIçµ±åˆ

```python
# app/api/v1/endpoints/chat.py

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import AsyncGenerator
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

class ChatRequest(BaseModel):
    query: str
    session_id: str = None

class ChatResponse(BaseModel):
    answer: str
    confidence: float
    sources: List[Dict]

@router.post("/chat", response_model=ChatResponse)
async def chat_endpoint(
    request: ChatRequest,
    rag_pipeline: RAGPipeline = Depends(get_rag_pipeline),
    agent_graph: MultiAgentRAGGraph = Depends(get_agent_graph)
):
    """
    7ãƒ•ã‚§ãƒ¼ã‚ºRAG + ãƒãƒ«ãƒLLMã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« Chat API

    1. RAG Pipeline (LangChain) ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    2. Multi-Agent Graph (LangGraph) ã§å›ç­”ç”Ÿæˆ
    """
    try:
        logger.info(f"Chat request: {request.query[:50]}...")

        # Phase 1-5: RAG Pipeline
        rag_result = rag_pipeline.invoke(request.query)

        # Phase 6: Confidence Check
        if rag_result["confidence"] < 0.5:
            raise HTTPException(
                status_code=404,
                detail="é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            )

        # Phase 7: Multi-Agent Ensemble
        agent_result = await agent_graph.invoke(
            query=request.query,
            context=rag_result["context"]
        )

        return ChatResponse(
            answer=agent_result["final_answer"],
            confidence=agent_result["confidence"],
            sources=[
                {
                    "filename": doc.metadata["filename"],
                    "content": doc.page_content[:200]
                }
                for doc in rag_result["ranked_docs"]
            ]
        )

    except Exception as e:
        logger.error(f"Chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/chat/stream")
async def chat_stream_endpoint(
    request: ChatRequest,
    agent_graph: MultiAgentRAGGraph = Depends(get_agent_graph)
):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°Chat API"""

    async def generate() -> AsyncGenerator[str, None]:
        async for chunk in agent_graph.astream(request.query):
            yield f"data: {chunk}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

### 6.2. Docker + Cloud Run ãƒ‡ãƒ—ãƒ­ã‚¤

```dockerfile
# Dockerfile

FROM python:3.11-slim

WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
COPY app/ ./app/

# ç’°å¢ƒå¤‰æ•°
ENV PYTHONPATH=/app
ENV PORT=8080

# FastAPIèµ·å‹•
CMD exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT}
```

```yaml
# cloudbuild.yaml

steps:
  # Docker Build
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/fractal-link/api:$SHORT_SHA', '.']

  # Docker Push
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/fractal-link/api:$SHORT_SHA']

  # Cloud Run Deploy
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    args:
      - 'gcloud'
      - 'run'
      - 'deploy'
      - 'fractal-link-api'
      - '--image=asia-northeast1-docker.pkg.dev/$PROJECT_ID/fractal-link/api:$SHORT_SHA'
      - '--region=asia-northeast1'
      - '--platform=managed'
      - '--allow-unauthenticated'
      - '--memory=4Gi'
      - '--cpu=2'
      - '--timeout=300s'
      - '--concurrency=80'
      - '--set-env-vars=ENVIRONMENT=production'

options:
  machineType: 'E2_HIGHCPU_8'
```

---

## 7. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°

### 7.1. LangSmithçµ±åˆï¼ˆæ¨å¥¨ï¼‰

```python
# app/core/tracing.py

import os
from langsmith import Client

def setup_langsmith():
    """LangSmith Tracing Setup"""
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY")
    os.environ["LANGCHAIN_PROJECT"] = "fractal-link-production"

    client = Client()
    return client
```

### 7.2. Cloud Loggingçµ±åˆ

```python
# app/core/logging_config.py

import logging
from google.cloud import logging as cloud_logging

def setup_logging():
    """Cloud Logging Setup"""
    client = cloud_logging.Client()
    client.setup_logging()

    logger = logging.getLogger("fractal-link")
    logger.setLevel(logging.INFO)

    return logger
```

### 7.3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
# app/core/monitoring.py

from functools import wraps
import time
from prometheus_client import Counter, Histogram
import logging

logger = logging.getLogger(__name__)

# Prometheus Metrics
rag_pipeline_duration = Histogram(
    'rag_pipeline_duration_seconds',
    'RAG Pipeline execution time'
)

agent_graph_duration = Histogram(
    'agent_graph_duration_seconds',
    'Agent Graph execution time'
)

chat_requests_total = Counter(
    'chat_requests_total',
    'Total chat requests'
)

def monitor_performance(metric: Histogram):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric.observe(duration)
                logger.info(f"{func.__name__} completed in {duration:.2f}s")
        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
@monitor_performance(rag_pipeline_duration)
async def execute_rag_pipeline(query: str):
    # RAGå‡¦ç†
    pass
```

---

## ã¾ã¨ã‚

### å®Ÿè£…å„ªå…ˆé †ä½

| Phase | å†…å®¹ | æœŸé–“ | ä¾å­˜é–¢ä¿‚ |
|-------|------|------|---------|
| Week 1-2 | LangChain RAG Pipelineå®Ÿè£… | 2é€±é–“ | Vertex AIè¨­å®šå®Œäº† |
| Week 3-4 | LangGraph Multi-Agentå®Ÿè£… | 2é€±é–“ | RAG Pipelineå®Œæˆ |
| Week 5 | FastAPIçµ±åˆ & ãƒ†ã‚¹ãƒˆ | 1é€±é–“ | ä¸¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Œæˆ |
| Week 6 | Cloud Run ãƒ‡ãƒ—ãƒ­ã‚¤ | 1é€±é–“ | çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº† |
| Week 7 | LangSmithçµ±åˆ & ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° | 1é€±é–“ | æœ¬ç•ªç’°å¢ƒç¨¼åƒ |
| Week 8 | Research-First Patternçµ±åˆ | 1é€±é–“ | åŸºæœ¬æ©Ÿèƒ½å®‰å®š |

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… Vertex AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šï¼ˆADR-005å‚ç…§ï¼‰
2. âœ… Cloud SQL MySQL 8.0ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆADR-004å‚ç…§ï¼‰
3. ğŸ”„ LangChain RAG Pipelineå®Ÿè£…ï¼ˆWeek 1-2ï¼‰
4. ğŸ”„ LangGraph Multi-Agentå®Ÿè£…ï¼ˆWeek 3-4ï¼‰

---

**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:**

- [ADR-007: AIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é¸å®š](../development/08_æŠ€è¡“çš„æ„æ€æ±ºå®šè¨˜éŒ².md#adr-007-aiãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é¸å®š)
- [Multi-Agent Development Guide](./multi_agent_development_guide.md)
- [GCP RAG Implementation Guide](../rag/08_gcp_rag_implementation_guide.md)
