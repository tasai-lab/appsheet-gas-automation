# RAGシステム使用ガイド

## 📖 概要

RAG (Retrieval-Augmented Generation) システムは、ベクトル検索とGemini 2.0を組み合わせて、PDFドキュメントに対する質問応答を実現します。

## 🚀 基本的な使い方

### 1. Gemini 2.0 Flash (高速・推奨)

```bash
python rag_search.py
```

起動後、質問を入力するだけで使えます:

```
💬 質問を入力してください: 浅井さんの強みは何ですか?
```

### 2. Gemini 2.0 Pro Experimental (最高品質)

重要な業務判断や複雑な分析が必要な場合:

```bash
python rag_search.py --model gemini-exp-1206
```

### 3. Gemini 2.0 Flash Thinking (思考プロセス表示)

回答の根拠を確認したい場合:

```bash
python rag_search.py --model gemini-2.0-flash-thinking-exp-01-21
```

## 🎯 モデル比較

| モデル | 速度 | 品質 | コスト | 推奨用途 |
|--------|------|------|--------|----------|
| **gemini-2.0-flash-exp** | ⚡⚡⚡ | ⭐⭐⭐ | 💰 | 日常的な質問応答 |
| **gemini-exp-1206** | ⚡⚡ | ⭐⭐⭐⭐⭐ | 💰💰💰 | 重要な業務判断 |
| **gemini-2.0-flash-thinking-exp-01-21** | ⚡⚡ | ⭐⭐⭐⭐ | 💰💰 | デバッグ・検証 |

## 🔧 詳細オプション

### 検索結果の数を調整

```bash
# 上位10件の文書を検索 (デフォルト: 5件)
python rag_search.py --top-k 10
```

### 類似度の閾値を調整

```bash
# 70%以上の類似度のみ使用 (デフォルト: 65%)
python rag_search.py --threshold 0.7

# 50%以上の類似度を使用 (より多くの文書を参照)
python rag_search.py --threshold 0.5
```

### 複数オプションの組み合わせ

```bash
python rag_search.py \
  --model gemini-exp-1206 \
  --top-k 10 \
  --threshold 0.6
```

## 💡 使用例

### 例1: 人事情報の検索

```
💬 質問を入力してください: クリフトンストレングスの結果で、戦略性が高い人は誰ですか?

🔍 関連文書を検索中 (上位5件, 閾値65%)...
✅ 4件の関連文書を発見

📑 検索結果:
  1. クリフトンストレングス_浅井.pdf (チャンク 2/5) - 類似度: 72.3%
  2. クリフトンストレングス_神馬.pdf (チャンク 1/5) - 類似度: 68.9%
  3. クリフトンストレングス_古谷.pdf (チャンク 2/5) - 類似度: 67.1%
  4. クリフトンストレングス_母.pdf (チャンク 1/5) - 類似度: 65.4%

🤖 gemini-2.0-flash-exp で回答を生成中...

🎯 回答:
======================================================================
文書1によると、浅井さんのTop 5の強みの中に「戦略性」が含まれています。
文書2の神馬さんも「戦略性」が上位に入っています。

一方、文書3の古谷さんと文書4のお母様のTop 5には「戦略性」は
含まれていないようです。

したがって、戦略性が高いのは浅井さんと神馬さんです。
======================================================================
```

### 例2: 複数文書からの情報統合

```
💬 質問を入力してください: チームメンバーの強みを比較して、プロジェクトの役割分担を提案してください

# Proモデルで高品質な分析
$ python rag_search.py --model gemini-exp-1206

🎯 回答:
======================================================================
各メンバーの強みを分析した結果、以下の役割分担を提案します:

【プロジェクトマネージャー】浅井さん
- 戦略性、学習欲、収集心により、全体戦略と情報管理に適しています

【技術リーダー】神馬さん
- 分析思考、戦略性により、技術的判断とアーキテクチャ設計に適しています

【チームビルダー】古谷さん  
- 共感性、調和性により、チームの調整役に適しています

各メンバーの詳細な強みは文書1-4に記載されています。
======================================================================
```

### 例3: 思考プロセスの確認

```
$ python rag_search.py --model gemini-2.0-flash-thinking-exp-01-21

💬 質問を入力してください: 浅井さんの学習欲について詳しく教えてください

🎯 回答:
======================================================================
<thinking>
まず文書1を確認すると、浅井さんのTop 5に「学習欲」が含まれている。
文書内の説明を探すと...
- 学習プロセス自体に魅力を感じる
- 結果よりもプロセスを重視
- 新しいことに興味を持つ
このような特徴が記載されている。
</thinking>

浅井さんの「学習欲」の特徴:
1. 学ぶプロセス自体を楽しむ
2. 結果だけでなく、学習の過程を大切にする
3. 常に新しい知識やスキルに興味を持つ
...
======================================================================
```

## 🔍 システムの動作

### 処理フロー

1. **質問の入力**: ユーザーが自然言語で質問
2. **ベクトル化**: gemini-embedding-001 で質問をベクトル化 (RETRIEVAL_QUERY)
3. **類似検索**: Vector Search で関連文書を検索 (DOT_PRODUCT_DISTANCE)
4. **フィルタリング**: 類似度65%以上の文書のみ取得
5. **コンテキスト構築**: GCSから実際のテキストを取得し、コンテキストを構築
6. **回答生成**: Gemini 2.0 に質問とコンテキストを渡して回答生成
7. **結果表示**: 検索結果と回答を表示

### 技術スタック

- **Embedding**: gemini-embedding-001 (3072次元)
- **Vector Search**: Vertex AI Vector Search
  - Distance: DOT_PRODUCT_DISTANCE
  - Threshold: 0.65 (65%以上)
  - Top-K: 5件 (デフォルト)
- **LLM**: Gemini 2.0 Flash/Pro
  - Temperature: 0.2 (正確性重視)
  - Max Output: 2048 tokens
- **Storage**: Google Cloud Storage
  - PDFファイル: gs://fractalautomations-documents/
  - Embeddings JSON: gs://fractalautomations-documents/embeddings/

## ⚙️ 設定のカスタマイズ

`rag_search.py` を編集して、以下をカスタマイズできます:

### プロンプトの調整

```python
# 回答スタイルを変更
prompt = f"""以下の文書を参照して、ユーザーの質問に日本語で回答してください。

【参照文書】
{context}

【ユーザーの質問】
{query}

【回答の指針】
- 簡潔かつ具体的に回答してください  # ← ここを変更
- ビジネス文書として適切な敬語を使用してください  # ← 追加
...
"""
```

### 温度(Temperature)の調整

```python
# より創造的な回答が欲しい場合
config=types.GenerateContentConfig(
    temperature=0.7,  # 0.2 → 0.7 に変更
    ...
)
```

### 検索パラメータの調整

```python
# より多くの文書を参照
results = search_similar_documents(
    query, 
    top_k=10,  # 5 → 10 に変更
    similarity_threshold=0.60  # 0.65 → 0.60 に変更
)
```

## 🐛 トラブルシューティング

### 「関連文書が見つかりません」が表示される

**原因**: 類似度が閾値(65%)に達しない

**対策**:
```bash
# 閾値を下げて試す
python rag_search.py --threshold 0.5

# より多くの候補を検索
python rag_search.py --top-k 20 --threshold 0.5
```

### 回答が不正確

**原因**: 
1. 検索結果に関連文書が含まれていない
2. コンテキストが不十分

**対策**:
```bash
# より多くの文書を参照
python rag_search.py --top-k 10

# より高品質なモデルを使用
python rag_search.py --model gemini-exp-1206 --top-k 10
```

### 処理が遅い

**原因**: Proモデルは処理時間が長い

**対策**:
```bash
# Flashモデルを使用
python rag_search.py --model gemini-2.0-flash-exp

# 検索文書数を減らす
python rag_search.py --top-k 3
```

## 📊 パフォーマンス目安

| モデル | 検索時間 | 回答生成時間 | 合計時間 |
|--------|----------|--------------|----------|
| Flash | 1-2秒 | 2-3秒 | **3-5秒** |
| Pro | 1-2秒 | 5-10秒 | **6-12秒** |
| Thinking | 1-2秒 | 8-15秒 | **9-17秒** |

※ネットワーク速度やドキュメント数により変動

## 🔐 セキュリティ

- **認証**: Application Default Credentials (ADC) を使用
- **アクセス制御**: GCS・Vertex AIのIAMで管理
- **データ保護**: すべての通信はHTTPS
- **クォータ管理**: Gemini API呼び出しは rate limiting 実装済み

## 📝 ベストプラクティス

### 1. モデル選択

- **日常業務**: Flash (高速・コスパ良)
- **重要判断**: Pro (最高品質)
- **デバッグ**: Thinking (根拠確認)

### 2. 質問の書き方

✅ **良い質問**:
```
「浅井さんの戦略性について、具体的な特徴を教えてください」
「チームメンバーの強みを比較して、プロジェクト役割を提案してください」
```

❌ **悪い質問**:
```
「浅井」 (曖昧すぎる)
「強み」 (具体性がない)
```

### 3. 閾値の調整

- **高精度優先**: `--threshold 0.7` (70%以上)
- **バランス型**: `--threshold 0.65` (デフォルト)
- **網羅性優先**: `--threshold 0.5` (50%以上)

## 🔗 関連ドキュメント

- [README.md](README.md) - システム全体の概要
- [QUOTA_HANDLING.md](QUOTA_HANDLING.md) - APIクォータ対策
- [BEST_PRACTICES_REPORT.md](BEST_PRACTICES_REPORT.md) - 実装ベストプラクティス

## 💬 フィードバック

質問や改善提案は浅井まで:
- Email: t.asai@fractal-group.co.jp
- GitHub: tasai-lab/vectorize-function
