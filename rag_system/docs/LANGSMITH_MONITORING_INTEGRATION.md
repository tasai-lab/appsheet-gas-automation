# LangSmithç›£è¦–çµ±åˆè¨­è¨ˆæ›¸

**æœ€çµ‚æ›´æ–°**: 2025-10-27
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: è¨­è¨ˆä¸­
**å¯¾è±¡ç’°å¢ƒ**: Production

---

## æ¦‚è¦

LangSmithã‚’RAG Medical Assistant APIã«çµ±åˆã—ã€LLMå‘¼ã³å‡ºã—ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã€ã‚³ã‚¹ãƒˆè¿½è·¡ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

---

## LangSmithã¨ã¯

**LangSmith** ã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã®ãŸã‚ã®è¦³æ¸¬æ€§ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚

**ä¸»è¦æ©Ÿèƒ½:**
- **ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°**: LLMå‘¼ã³å‡ºã—ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å¯è¦–åŒ–
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨A/Bãƒ†ã‚¹ãƒˆ
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã€ã‚³ã‚¹ãƒˆ
- **ãƒ‡ãƒãƒƒã‚°**: ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã€å…¥å‡ºåŠ›ã®è©³ç´°è¨˜éŒ²
- **è©•ä¾¡**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å“è³ªè©•ä¾¡ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TB
    User([ãƒ¦ãƒ¼ã‚¶ãƒ¼]) --> Frontend[Next.js Frontend]
    Frontend --> Backend[FastAPI Backend]

    Backend --> Trace1[ãƒˆãƒ¬ãƒ¼ã‚¹é–‹å§‹]
    Trace1 --> Preprocess[Query Preprocessing]
    Preprocess --> BM25[BM25 Filtering]
    BM25 --> Embed[Embeddingç”Ÿæˆ]
    Embed --> Dense[Dense Retrieval]
    Dense --> Rerank[Reranking]
    Rerank --> Generate[Geminiç”Ÿæˆ]
    Generate --> Trace2[ãƒˆãƒ¬ãƒ¼ã‚¹çµ‚äº†]

    Trace1 -.->|é€ä¿¡| LangSmith[LangSmith]
    Embed -.->|è¨˜éŒ²| LangSmith
    Generate -.->|è¨˜éŒ²| LangSmith
    Trace2 -.->|é€ä¿¡| LangSmith

    LangSmith --> Dashboard[LangSmith Dashboard]
    Dashboard --> Analysis[åˆ†æãƒ»æœ€é©åŒ–]

    style LangSmith fill:#7B68EE
    style Dashboard fill:#4A90E2
    style Analysis fill:#50C878
```

---

## ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°è¨­è¨ˆ

### 1. ãƒˆãƒ¬ãƒ¼ã‚¹éšå±¤

```
Root Trace: /chat/stream
â”œâ”€â”€ Span 1: Query Preprocessing
â”œâ”€â”€ Span 2: BM25 Filtering (Spreadsheet)
â”œâ”€â”€ Span 3: Embedding Generation (Vertex AI)
â”‚   â””â”€â”€ LLM Call: Vertex AI Embeddings
â”œâ”€â”€ Span 4: Dense Retrieval (Cosine Similarity)
â”œâ”€â”€ Span 5: Reranking (Vertex AI Ranking API)
â”‚   â””â”€â”€ LLM Call: Vertex AI Ranking
â””â”€â”€ Span 6: Response Generation (Gemini)
    â””â”€â”€ LLM Call: Vertex AI Gemini
```

### 2. è¨˜éŒ²ã•ã‚Œã‚‹æƒ…å ±

å„ã‚¹ãƒ‘ãƒ³ã§ä»¥ä¸‹ã‚’è¨˜éŒ²:

| é …ç›® | å†…å®¹ | ä¾‹ |
|------|------|-----|
| **å…¥åŠ›** | ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | "åˆ©ç”¨è€…Aã®è¡€åœ§ã¯?" |
| **å‡ºåŠ›** | æ¤œç´¢çµæœã€ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ | "è¡€åœ§: 120/80 mmHg" |
| **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿** | ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ID | client_12345 |
| **ã‚¿ã‚¤ãƒŸãƒ³ã‚°** | é–‹å§‹æ™‚åˆ»ã€çµ‚äº†æ™‚åˆ»ã€æ‰€è¦æ™‚é–“ | 2.3ç§’ |
| **ãƒˆãƒ¼ã‚¯ãƒ³æ•°** | å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã€å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ | 1000 / 500 |
| **ã‚³ã‚¹ãƒˆ** | APIå‘¼ã³å‡ºã—ã‚³ã‚¹ãƒˆ | Â¥1.295 |
| **ã‚¨ãƒ©ãƒ¼** | ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ | TimeoutError |

---

## å®Ÿè£…è©³ç´°

### Phase 1: LangSmithè¨­å®š

#### 1.1 LangSmithã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®š

```bash
# LangSmith Console (https://smith.langchain.com/)
1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ: "RAG-Medical-Assistant"
2. API Keyå–å¾—: Settings > API Keys
3. ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
```

#### 1.2 ç’°å¢ƒå¤‰æ•°è¨­å®š

**`backend/.env`:**
```bash
# ================================================================
# LangSmithè¨­å®š
# ================================================================
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=ls__...
LANGCHAIN_PROJECT=RAG-Medical-Assistant

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆå…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½•%ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰
LANGSMITH_SAMPLING_RATE=1.0  # 1.0 = 100%ï¼ˆå…¨ã¦ï¼‰ã€0.1 = 10%
```

---

### Phase 2: Backendå®Ÿè£…

#### 2.1 ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è¿½åŠ 

**`backend/requirements.txt`ï¼ˆè¿½åŠ ï¼‰:**
```txt
langsmith>=0.1.0
langchain-core>=0.1.0
```

#### 2.2 LangSmithã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–

**`backend/app/services/langsmith_service.py`ï¼ˆæ–°è¦ä½œæˆï¼‰:**
```python
"""
LangSmithç›£è¦–ã‚µãƒ¼ãƒ“ã‚¹

LLMå‘¼ã³å‡ºã—ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
import os
from typing import Optional, Dict, Any, List
from datetime import datetime
import random

from langsmith import Client
from langsmith.run_helpers import traceable
from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«LangSmithã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
_langsmith_client: Optional[Client] = None


def get_langsmith_client() -> Optional[Client]:
    """
    LangSmithã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—

    Returns:
        LangSmith Clientï¼ˆç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯Noneï¼‰
    """
    global _langsmith_client

    if not settings.langchain_tracing_v2:
        logger.info("LangSmith tracing is disabled")
        return None

    if _langsmith_client is not None:
        return _langsmith_client

    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰åˆæœŸåŒ–ï¼ˆlangsmith SDKãŒè‡ªå‹•çš„ã«èª­ã¿è¾¼ã¿ï¼‰
        _langsmith_client = Client()
        logger.info("âœ… LangSmith client initialized")
        return _langsmith_client

    except Exception as e:
        logger.error(f"Failed to initialize LangSmith client: {e}", exc_info=True)
        return None


def should_trace() -> bool:
    """
    ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ã„ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã¹ãã‹åˆ¤å®š

    Returns:
        ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹å ´åˆTrue
    """
    if not settings.langchain_tracing_v2:
        return False

    sampling_rate = settings.langsmith_sampling_rate
    return random.random() < sampling_rate


class LangSmithTracer:
    """
    LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, client: Optional[Client] = None):
        """
        åˆæœŸåŒ–

        Args:
            client: LangSmith Clientï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•å–å¾—ï¼‰
        """
        self.client = client or get_langsmith_client()
        self.enabled = self.client is not None and should_trace()

    @traceable(run_type="chain", name="RAG Chat Pipeline")
    async def trace_chat_pipeline(
        self,
        query: str,
        client_id: Optional[str] = None,
        user_email: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ãƒãƒ£ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            client_id: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID
            user_email: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
            **kwargs: ãã®ä»–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

        Returns:
            å®Ÿè¡Œçµæœ
        """
        # ã“ã®é–¢æ•°å†…ã§å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ã¦ã®@traceableé–¢æ•°ãŒ
        # è‡ªå‹•çš„ã«å­ã‚¹ãƒ‘ãƒ³ã¨ã—ã¦è¨˜éŒ²ã•ã‚Œã‚‹
        pass

    @traceable(run_type="llm", name="Vertex AI Embeddings")
    async def trace_embeddings(
        self,
        text: str,
        model: str = "gemini-embedding-001",
        **kwargs
    ) -> List[float]:
        """
        Embeddingç”Ÿæˆã‚’ãƒˆãƒ¬ãƒ¼ã‚¹

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            model: ãƒ¢ãƒ‡ãƒ«å
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        # å®Ÿéš›ã®å®Ÿè£…ã¯ vertex_ai_service.py ã«å§”è­²
        # ã“ã“ã§ã¯ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®ã¿
        pass

    @traceable(run_type="llm", name="Vertex AI Gemini Generation")
    async def trace_generation(
        self,
        prompt: str,
        model: str = "gemini-2.5-flash",
        temperature: float = 0.3,
        **kwargs
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ãƒˆãƒ¬ãƒ¼ã‚¹

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            model: ãƒ¢ãƒ‡ãƒ«å
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ
        """
        # å®Ÿéš›ã®å®Ÿè£…ã¯ gemini_service.py ã«å§”è­²
        pass

    @traceable(run_type="retriever", name="Hybrid Search")
    async def trace_search(
        self,
        query: str,
        top_k: int = 10,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: å–å¾—ä»¶æ•°
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
        """
        pass

    def log_feedback(
        self,
        run_id: str,
        score: float,
        comment: Optional[str] = None
    ):
        """
        ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²

        Args:
            run_id: ãƒˆãƒ¬ãƒ¼ã‚¹ã®Run ID
            score: ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
            comment: ã‚³ãƒ¡ãƒ³ãƒˆ
        """
        if not self.enabled or not self.client:
            return

        try:
            self.client.create_feedback(
                run_id=run_id,
                key="user_score",
                score=score,
                comment=comment
            )
            logger.info(f"Feedback logged for run {run_id}: {score}")
        except Exception as e:
            logger.error(f"Failed to log feedback: {e}", exc_info=True)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼
_tracer: Optional[LangSmithTracer] = None


def get_tracer() -> LangSmithTracer:
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ã‚’å–å¾—

    Returns:
        LangSmithTracer
    """
    global _tracer
    if _tracer is None:
        _tracer = LangSmithTracer()
    return _tracer
```

#### 2.3 æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®çµ±åˆ

**`backend/app/services/vertex_ai.py`ï¼ˆä¿®æ­£ï¼‰:**
```python
from app.services.langsmith_service import get_tracer
from langsmith.run_helpers import traceable

class VertexAIService:
    # ... æ—¢å­˜ã‚³ãƒ¼ãƒ‰ ...

    @traceable(run_type="llm", name="Vertex AI Embeddings API")
    def generate_query_embedding(
        self,
        query: str,
        output_dimensionality: Optional[int] = None
    ) -> List[float]:
        """
        ã‚¯ã‚¨ãƒªEmbeddingã‚’ç”Ÿæˆï¼ˆLangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ä»˜ãï¼‰

        Args:
            query: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            output_dimensionality: å‡ºåŠ›æ¬¡å…ƒæ•°

        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        cache = get_cache_service()
        cache_key = hashlib.sha256(f"{query}_{output_dimensionality}".encode()).hexdigest()

        if settings.cache_enabled:
            cached_embedding = cache.get("embeddings", cache_key)
            if cached_embedding is not None:
                logger.info(f"âœ… Using cached query embedding")
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚‚LangSmithã«è¨˜éŒ²
                return cached_embedding

        # â˜…â˜…â˜… Vertex AI APIå‘¼ã³å‡ºã—: 1å›ã®ã¿å®Ÿè¡Œ â˜…â˜…â˜…
        logger.info(f"ğŸ“¡ Generating query embedding via Vertex AI...")

        start_time = time.time()

        try:
            embedding = self._call_embeddings_api(query, output_dimensionality)
            elapsed_time = time.time() - start_time

            logger.info(f"âœ… Embedding generated in {elapsed_time:.2f}s")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if settings.cache_enabled:
                cache.set("embeddings", cache_key, embedding, settings.cache_embeddings_ttl)

            return embedding

        except Exception as e:
            logger.error(f"Embedding generation error: {e}", exc_info=True)
            raise

    def _call_embeddings_api(
        self,
        query: str,
        output_dimensionality: Optional[int] = None
    ) -> List[float]:
        """
        å®Ÿéš›ã®Embeddings APIå‘¼ã³å‡ºã—ï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        @traceable ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã¯ä»˜ã‘ãªã„ï¼ˆè¦ªé–¢æ•°ã§ãƒˆãƒ¬ãƒ¼ã‚¹æ¸ˆã¿ï¼‰
        """
        # ... æ—¢å­˜ã®å®Ÿè£… ...
```

**`backend/app/services/gemini_service.py`ï¼ˆä¿®æ­£ï¼‰:**
```python
from langsmith.run_helpers import traceable

class GeminiService:
    # ... æ—¢å­˜ã‚³ãƒ¼ãƒ‰ ...

    @traceable(run_type="llm", name="Vertex AI Gemini Generation")
    async def generate_response(
        self,
        prompt: str,
        search_results: List[Dict[str, Any]],
        stream: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        RAGå›ç­”ç”Ÿæˆï¼ˆLangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ä»˜ãï¼‰

        Args:
            prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            search_results: æ¤œç´¢çµæœ
            stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–

        Yields:
            ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯
        """
        logger.info("=== Gemini Response Generation ===")
        logger.info(f"Prompt: {prompt[:100]}...")
        logger.info(f"Search Results Count: {len(search_results)}")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        full_prompt = self._build_prompt(prompt, search_results)

        # â˜…â˜…â˜… Vertex AI APIå‘¼ã³å‡ºã—: 1å›ã®ã¿å®Ÿè¡Œ â˜…â˜…â˜…
        try:
            if stream:
                async for chunk in self._call_gemini_api_stream(full_prompt):
                    yield chunk
            else:
                result = await self._call_gemini_api(full_prompt)
                yield result

        except Exception as e:
            logger.error(f"Generation error: {e}", exc_info=True)
            raise
```

#### 2.4 ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµ±åˆ

**`backend/app/routers/chat.py`ï¼ˆä¿®æ­£ï¼‰:**
```python
from app.services.langsmith_service import get_tracer
from langsmith.run_helpers import traceable

@router.post("/stream")
@traceable(run_type="chain", name="Chat Stream Endpoint")
async def chat_stream(
    request: ChatRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ãƒãƒ£ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆLangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ä»˜ãï¼‰

    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        current_user: èªè¨¼æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
    """
    logger.info("=" * 60)
    logger.info("ğŸ“¨ Chat Stream Request")
    logger.info(f"Client: {request.clientId}")
    logger.info(f"Query: {request.query}")
    logger.info(f"User: {current_user.get('email')}")
    logger.info("=" * 60)

    # LangSmithãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
    tracer = get_tracer()
    if tracer.enabled:
        # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        from langsmith import get_current_run_tree
        run_tree = get_current_run_tree()
        if run_tree:
            run_tree.extra = {
                "client_id": request.clientId,
                "user_email": current_user.get('email'),
                "streaming": request.streaming,
                "environment": settings.environment
            }

    # ... æ—¢å­˜ã®å®Ÿè£… ...
```

#### 2.5 è¨­å®šè¿½åŠ 

**`backend/app/config.py`ï¼ˆè¿½åŠ ï¼‰:**
```python
# LangSmithè¨­å®š
langchain_tracing_v2: bool = False  # LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æœ‰åŠ¹åŒ–
langchain_endpoint: str = "https://api.smith.langchain.com"
langchain_api_key: str = ""
langchain_project: str = "RAG-Medical-Assistant"
langsmith_sampling_rate: float = 1.0  # 0.0-1.0ï¼ˆ1.0 = å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰
```

---

### Phase 3: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½

#### 3.1 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

**`backend/app/routers/feedback.py`ï¼ˆæ–°è¦ä½œæˆï¼‰:**
```python
"""
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’LangSmithã«è¨˜éŒ²ã—ã¾ã™ã€‚
"""

import logging
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field

from app.middleware.auth import get_current_user
from app.services.langsmith_service import get_tracer

logger = logging.getLogger(__name__)
router = APIRouter()


class FeedbackRequest(BaseModel):
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    run_id: str = Field(..., description="ãƒˆãƒ¬ãƒ¼ã‚¹ã®Run ID")
    score: float = Field(..., ge=0.0, le=1.0, description="ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰")
    comment: str = Field(None, description="ã‚³ãƒ¡ãƒ³ãƒˆ")


@router.post("/feedback")
async def submit_feedback(
    request: FeedbackRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡

    Args:
        request: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        current_user: èªè¨¼æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼
    """
    logger.info(f"Feedback from {current_user.get('email')}: {request.score}")

    tracer = get_tracer()

    try:
        tracer.log_feedback(
            run_id=request.run_id,
            score=request.score,
            comment=request.comment
        )

        return {
            "status": "success",
            "message": "Feedback recorded"
        }

    except Exception as e:
        logger.error(f"Failed to record feedback: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to record feedback")
```

**`backend/app/main.py`ï¼ˆãƒ«ãƒ¼ã‚¿ãƒ¼ç™»éŒ²ï¼‰:**
```python
from app.routers import chat, clients, health, feedback

app.include_router(feedback.router, prefix="/feedback", tags=["Feedback"])
```

#### 3.2 Frontend ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯UI

**`frontend/src/components/Message.tsx`ï¼ˆè¿½åŠ ï¼‰:**
```typescript
import { useState } from 'react';

interface MessageProps {
  // ... æ—¢å­˜ã®props
  runId?: string;  // LangSmith Run ID
}

export default function Message({ message, runId }: MessageProps) {
  const [feedbackScore, setFeedbackScore] = useState<number | null>(null);

  const submitFeedback = async (score: number) => {
    if (!runId) return;

    try {
      const response = await fetch(`${API_URL}/feedback`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${idToken}`,
        },
        body: JSON.stringify({
          run_id: runId,
          score: score / 5.0,  // 5æ®µéš â†’ 0.0-1.0ã«å¤‰æ›
        }),
      });

      if (response.ok) {
        setFeedbackScore(score);
        console.log('Feedback submitted:', score);
      }
    } catch (error) {
      console.error('Failed to submit feedback:', error);
    }
  };

  return (
    <div className="message">
      {/* ... æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º ... */}

      {message.role === 'assistant' && runId && (
        <div className="feedback-buttons">
          <span className="text-sm text-muted-foreground">ã“ã®å›ç­”ã¯å½¹ã«ç«‹ã¡ã¾ã—ãŸã‹ï¼Ÿ</span>
          {[1, 2, 3, 4, 5].map((score) => (
            <button
              key={score}
              onClick={() => submitFeedback(score)}
              className={`feedback-btn ${feedbackScore === score ? 'selected' : ''}`}
            >
              {score}â­
            </button>
          ))}
        </div>
      )}
    </div>
  );
}
```

---

## ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

### 1. LangSmith Dashboard

**è‡ªå‹•çš„ã«è¨˜éŒ²ã•ã‚Œã‚‹æŒ‡æ¨™:**
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼ˆæ™‚ç³»åˆ—ï¼‰
- å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
- ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ï¼ˆå…¥åŠ›/å‡ºåŠ›ï¼‰
- ã‚¨ãƒ©ãƒ¼ç‡
- ã‚³ã‚¹ãƒˆæ¨å®š
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢

**ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**
- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥ä½¿ç”¨é‡
- æ¤œç´¢ç²¾åº¦ï¼ˆæ¤œç´¢çµæœæ•°ã€å†ãƒ©ãƒ³ã‚¯ã‚¹ã‚³ã‚¢ï¼‰
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ï¼ˆã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°ã§è¨˜éŒ²ï¼‰

### 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†

**LangSmith Hubæ©Ÿèƒ½:**
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- A/Bãƒ†ã‚¹ãƒˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒï¼‰
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒŸãƒƒãƒˆå±¥æ­´

**æ´»ç”¨ä¾‹:**
```python
# LangSmith Hubã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
from langsmith import hub

prompt_template = hub.pull("rag-medical-assistant/main")
prompt = prompt_template.format(query=query, context=context)
```

---

## ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š

### 1. ç’°å¢ƒå¤‰æ•°è¨­å®š

**æœ¬ç•ªç’°å¢ƒï¼ˆCloud Runï¼‰:**
```bash
# Secret Managerã«ä¿å­˜
echo -n "ls__your-api-key" | gcloud secrets create LANGCHAIN_API_KEY --data-file=-

# Cloud Runç’°å¢ƒå¤‰æ•°è¨­å®š
gcloud run services update rag-backend \
  --set-env-vars LANGCHAIN_TRACING_V2=true \
  --set-env-vars LANGCHAIN_PROJECT=RAG-Medical-Assistant \
  --set-env-vars LANGSMITH_SAMPLING_RATE=0.1 \  # æœ¬ç•ªã§ã¯10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¨å¥¨
  --set-secrets LANGCHAIN_API_KEY=LANGCHAIN_API_KEY:latest
```

**é–‹ç™ºç’°å¢ƒ:**
```bash
# backend/.env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__your-api-key
LANGCHAIN_PROJECT=RAG-Medical-Assistant-Dev
LANGSMITH_SAMPLING_RATE=1.0  # é–‹ç™ºã§ã¯100%
```

### 2. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆèª¿æ•´

**æ¨å¥¨è¨­å®š:**

| ç’°å¢ƒ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ | ç†ç”± |
|------|------------------|------|
| Development | 100% (1.0) | å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ‡ãƒãƒƒã‚° |
| Staging | 50% (0.5) | ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨ãƒ‡ãƒ¼ã‚¿åé›†ã®ãƒãƒ©ãƒ³ã‚¹ |
| Production (ä½ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯) | 100% (1.0) | å…¨ãƒ‡ãƒ¼ã‚¿è¨˜éŒ² |
| Production (é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯) | 10-20% (0.1-0.2) | ã‚³ã‚¹ãƒˆå‰Šæ¸› |

---

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

### 1. å€‹äººæƒ…å ±ã®ãƒã‚¹ã‚­ãƒ³ã‚°

**å®Ÿè£…ä¾‹:**
```python
import re

def mask_personal_info(text: str) -> str:
    """
    å€‹äººæƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°

    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        ãƒã‚¹ã‚­ãƒ³ã‚°æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
    """
    # åˆ©ç”¨è€…åã®ãƒã‚¹ã‚­ãƒ³ã‚°
    text = re.sub(r'åˆ©ç”¨è€…[A-Z]', 'åˆ©ç”¨è€…[MASKED]', text)

    # é›»è©±ç•ªå·ã®ãƒã‚¹ã‚­ãƒ³ã‚°
    text = re.sub(r'\d{2,4}-\d{2,4}-\d{4}', 'XXX-XXXX-XXXX', text)

    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒã‚¹ã‚­ãƒ³ã‚°
    text = re.sub(r'\S+@\S+\.\S+', '[EMAIL]', text)

    return text

# LangSmithè¨˜éŒ²å‰ã«é©ç”¨
@traceable(run_type="llm")
async def trace_generation_masked(prompt: str, **kwargs):
    masked_prompt = mask_personal_info(prompt)
    # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã«ã¯ãƒã‚¹ã‚¯æ¸ˆã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    ...
```

### 2. ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“

**LangSmithè¨­å®š:**
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 90æ—¥é–“ä¿æŒ
- ã‚«ã‚¹ã‚¿ãƒ : ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã§å¤‰æ›´å¯èƒ½

---

## ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š

### LangSmithæ–™é‡‘

**ç„¡æ–™ãƒ—ãƒ©ãƒ³:**
- 5,000ãƒˆãƒ¬ãƒ¼ã‚¹/æœˆã¾ã§ç„¡æ–™
- åŸºæœ¬çš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½

**Developerãƒ—ãƒ©ãƒ³ ($39/æœˆ):**
- 50,000ãƒˆãƒ¬ãƒ¼ã‚¹/æœˆ
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†æ©Ÿèƒ½
- ãƒãƒ¼ãƒ ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

**Enterpriseãƒ—ãƒ©ãƒ³:**
- ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ã‚¹æ•°
- å°‚ç”¨ã‚µãƒãƒ¼ãƒˆ
- ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹å±•é–‹

### æ¨å®šä½¿ç”¨é‡

**æƒ³å®šãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯: 1,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ—¥**

| ã‚·ãƒŠãƒªã‚ª | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ | æœˆé–“ãƒˆãƒ¬ãƒ¼ã‚¹æ•° | ãƒ—ãƒ©ãƒ³æ¨å¥¨ |
|---------|------------------|--------------|----------|
| ä½è² è· | 100% | 30,000 | Developer |
| ä¸­è² è· | 50% | 15,000 | Developer |
| é«˜è² è· | 10% | 3,000 | Free |

---

## ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆè¨ˆç”»

### Phase 1: é–‹ç™ºç’°å¢ƒãƒ†ã‚¹ãƒˆï¼ˆ1é€±é–“ï¼‰
- [ ] LangSmithã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®š
- [ ] Backendçµ±åˆå®Ÿè£…
- [ ] ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
- [ ] ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª

### Phase 2: ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒï¼ˆ1é€±é–“ï¼‰
- [ ] ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆèª¿æ•´
- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
- [ ] ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ

### Phase 3: æœ¬ç•ªç’°å¢ƒï¼ˆæ®µéšçš„ï¼‰
- [ ] 10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é–‹å§‹
- [ ] 1é€±é–“ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
- [ ] ã‚³ã‚¹ãƒˆãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
- [ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ–

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ: ãƒˆãƒ¬ãƒ¼ã‚¹ãŒè¨˜éŒ²ã•ã‚Œãªã„

**åŸå› **: API Keyæœªè¨­å®šã¾ãŸã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ0

**è§£æ±ºç­–:**
```bash
# ç’°å¢ƒå¤‰æ•°ç¢ºèª
echo $LANGCHAIN_API_KEY
echo $LANGCHAIN_TRACING_V2

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆç¢ºèª
echo $LANGSMITH_SAMPLING_RATE
```

### å•é¡Œ: ãƒˆãƒ¬ãƒ¼ã‚¹ãŒé…å»¶ã™ã‚‹

**åŸå› **: LangSmith APIã¸ã®é€ä¿¡ãŒãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°

**è§£æ±ºç­–**: éåŒæœŸé€ä¿¡ã‚’ä½¿ç”¨ï¼ˆlangsmith SDKã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§éåŒæœŸï¼‰

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [LangSmith Python SDK](https://github.com/langchain-ai/langsmith-sdk)
- [Tracing with LangSmith](https://docs.smith.langchain.com/tracing)
- [Prompt Management](https://docs.smith.langchain.com/prompt-hub)

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**
1. LangSmithã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®š
2. Backendå®Ÿè£…
3. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ
4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
5. ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ãƒ‡ãƒ—ãƒ­ã‚¤
6. æœ¬ç•ªãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ

---

**æœ€çµ‚æ›´æ–°**: 2025-10-27
**ä½œæˆè€…**: Claude Code
**ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å¿…é ˆï¼ˆå®Ÿè£…å‰ï¼‰
