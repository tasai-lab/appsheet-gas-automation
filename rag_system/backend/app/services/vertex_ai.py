"""
Vertex AI ã‚µãƒ¼ãƒ“ã‚¹

Embeddingsç”Ÿæˆã¨Ranking APIã‚’æä¾›ã—ã¾ã™ã€‚
"""

import hashlib
import logging
from typing import List, Optional

from google.auth import default
from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel

from app.config import get_settings
from app.services.cache_service import get_cache_service

logger = logging.getLogger(__name__)
settings = get_settings()


class VertexAIClient:
    """Vertex AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        # Google Cloudèªè¨¼
        credentials, project = default()

        # Vertex AIåˆæœŸåŒ–
        aiplatform.init(
            project=settings.gcp_project_id,
            location=settings.gcp_location,
            credentials=credentials
        )

        # Embeddings ãƒ¢ãƒ‡ãƒ«
        self.embedding_model = TextEmbeddingModel.from_pretrained(
            settings.vertex_ai_embeddings_model
        )

        logger.info(
            f"Vertex AI initialized - Project: {settings.gcp_project_id}, "
            f"Location: {settings.gcp_location}, "
            f"Embeddings Model: {settings.vertex_ai_embeddings_model}"
        )

    def generate_embeddings(
        self,
        texts: List[str],
        task_type: str = "RETRIEVAL_DOCUMENT",
        output_dimensionality: Optional[int] = None
    ) -> List[List[float]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®Embeddingsã‚’ç”Ÿæˆ

        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
                - RETRIEVAL_QUERY: ã‚¯ã‚¨ãƒªç”¨
                - RETRIEVAL_DOCUMENT: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨
                - SEMANTIC_SIMILARITY: é¡ä¼¼åº¦è¨ˆç®—ç”¨
                - CLASSIFICATION: åˆ†é¡ç”¨
            output_dimensionality: å‡ºåŠ›æ¬¡å…ƒæ•° (768 or 3072, Noneã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)

        Returns:
            Embeddingsã®ãƒªã‚¹ãƒˆ
        """
        try:
            # å…¥åŠ›ã®æº–å‚™
            inputs = [
                TextEmbeddingInput(text=text, task_type=task_type)
                for text in texts
            ]

            # Embeddingsç”Ÿæˆ
            kwargs = {}
            if output_dimensionality:
                kwargs['output_dimensionality'] = output_dimensionality

            embeddings = self.embedding_model.get_embeddings(inputs, **kwargs)

            # ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
            vectors = [embedding.values for embedding in embeddings]

            logger.debug(
                f"Generated {len(vectors)} embeddings - "
                f"Task: {task_type}, Dim: {output_dimensionality or 'default'}"
            )

            return vectors

        except Exception as e:
            logger.error(f"Embeddings generation failed: {e}", exc_info=True)
            raise

    def generate_query_embedding(
        self,
        query: str,
        output_dimensionality: Optional[int] = None
    ) -> List[float]:
        """
        ã‚¯ã‚¨ãƒªç”¨ã®Embeddingã‚’ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰

        Args:
            query: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            output_dimensionality: å‡ºåŠ›æ¬¡å…ƒæ•°

        Returns:
            Embeddingãƒ™ã‚¯ãƒˆãƒ«
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ï¼‰
        cache = get_cache_service()
        cache_key = hashlib.sha256(f"{query}_{output_dimensionality}".encode()).hexdigest()

        if settings.cache_enabled:
            cached_embedding = cache.get("embeddings", cache_key)
            if cached_embedding is not None:
                logger.info(f"âœ… Using cached query embedding for: {query[:50]}...")
                return cached_embedding

        # â˜…â˜…â˜… Vertex AI APIå‘¼ã³å‡ºã—: 1å›ã®ã¿å®Ÿè¡Œ â˜…â˜…â˜…
        logger.info(f"ğŸ“¡ Generating query embedding for: {query[:50]}...")
        vectors = self.generate_embeddings(
            texts=[query],
            task_type="RETRIEVAL_QUERY",
            output_dimensionality=output_dimensionality
        )
        embedding = vectors[0]

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if settings.cache_enabled:
            cache.set("embeddings", cache_key, embedding, settings.cache_embeddings_ttl)
            logger.info(f"ğŸ’¾ Cached query embedding (TTL: {settings.cache_embeddings_ttl}s)")

        return embedding

    def generate_document_embeddings(
        self,
        documents: List[str],
        output_dimensionality: Optional[int] = None
    ) -> List[List[float]]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ã®Embeddingsã‚’ç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰

        Args:
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            output_dimensionality: å‡ºåŠ›æ¬¡å…ƒæ•°

        Returns:
            Embeddingsã®ãƒªã‚¹ãƒˆ
        """
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²å‡¦ç†ï¼ˆAPIã®åˆ¶é™ã«å¯¾å¿œï¼‰
        batch_size = 250  # Vertex AI Embeddings APIã®åˆ¶é™
        all_embeddings = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_embeddings = self.generate_embeddings(
                texts=batch,
                task_type="RETRIEVAL_DOCUMENT",
                output_dimensionality=output_dimensionality
            )
            all_embeddings.extend(batch_embeddings)

        logger.info(f"Generated embeddings for {len(documents)} documents")
        return all_embeddings


# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
_vertex_ai_client: Optional[VertexAIClient] = None


def get_vertex_ai_client() -> VertexAIClient:
    """
    Vertex AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰

    Returns:
        VertexAIClient: Vertex AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """
    global _vertex_ai_client
    if _vertex_ai_client is None:
        _vertex_ai_client = VertexAIClient()
    return _vertex_ai_client
