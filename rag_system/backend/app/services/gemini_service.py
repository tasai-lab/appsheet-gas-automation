"""
Geminiç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹

Vertex AI Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
æ€è€ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆThinking Modeï¼‰ã«å¯¾å¿œã€‚
"""

import asyncio
import logging
import os
from typing import List, Dict, Any, AsyncGenerator, Optional

import vertexai
from vertexai.generative_models import GenerativeModel, Content, Part
from google.auth import default

# Google Gen AI SDK (æ€è€ƒãƒ¢ãƒ¼ãƒ‰ç”¨)
from google import genai
from google.genai import types

from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class GeminiService:
    """Geminiç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        # Google Cloudèªè¨¼
        credentials, project = default()

        # Vertex AIåˆæœŸåŒ–
        vertexai.init(
            project=settings.gcp_project_id,
            location=settings.gcp_location,
            credentials=credentials
        )

        # Gemini ãƒ¢ãƒ‡ãƒ« (å¾“æ¥ã®vertexai SDK)
        self.model = GenerativeModel(settings.vertex_ai_generation_model)

        # Google Gen AI Client (æ€è€ƒãƒ¢ãƒ¼ãƒ‰ç”¨)
        # Vertex AIãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        os.environ['GOOGLE_CLOUD_PROJECT'] = settings.gcp_project_id
        os.environ['GOOGLE_CLOUD_LOCATION'] = settings.gcp_location
        os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'

        try:
            self.genai_client = genai.Client(
                vertexai=True,
                project=settings.gcp_project_id,
                location=settings.gcp_location
            )
            logger.info(
                f"Gemini Service initialized - "
                f"Model: {settings.vertex_ai_generation_model}, "
                f"Thinking Mode: {settings.vertex_ai_enable_thinking}"
            )
        except Exception as e:
            logger.warning(f"Failed to initialize Gen AI client: {e}. Falling back to vertexai SDK only.")
            self.genai_client = None

    async def generate_response(
        self,
        query: str,
        context: List[Dict[str, Any]],
        history: Optional[List[Dict[str, Any]]] = None,
        stream: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        å¿œç­”ã‚’ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ + ä¼šè©±å±¥æ­´å¯¾å¿œï¼‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            context: æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆKnowledgeItemsã®ãƒªã‚¹ãƒˆï¼‰
            history: ä¼šè©±å±¥æ­´ï¼ˆæœ€æ–°10ä»¶ç¨‹åº¦ï¼‰
            stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–

        Yields:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆå±¥æ­´å¯¾å¿œï¼‰
            prompt = self._build_prompt_with_history(query, context, history)

            logger.info(
                f"ğŸš€ Starting Gemini generation - "
                f"Query: {query[:50]}..., Context: {len(context)} items, "
                f"Stream: {stream}, Thinking Mode: {settings.vertex_ai_enable_thinking}"
            )

            # æ€è€ƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã€Gen AI ClientãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
            if settings.vertex_ai_enable_thinking and self.genai_client is not None:
                async for chunk in self._generate_with_thinking(prompt, stream):
                    yield chunk
                return

            # å¾“æ¥ã®Vertex AI SDKã‚’ä½¿ç”¨ï¼ˆæ€è€ƒãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            # ç”Ÿæˆè¨­å®š
            generation_config = {
                'temperature': settings.vertex_ai_temperature,
                'max_output_tokens': settings.vertex_ai_max_output_tokens,
                'top_p': settings.vertex_ai_top_p,
                'top_k': settings.vertex_ai_top_k
            }

            # â˜…â˜…â˜… Vertex AI APIå‘¼ã³å‡ºã—: 1å›ã®ã¿å®Ÿè¡Œ â˜…â˜…â˜…
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: 120ç§’ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šé•·ã‚ã«è¨­å®šï¼‰
            timeout_seconds = 120

            try:
                if stream:
                    logger.info("ğŸ“¡ Calling Gemini API with streaming...")

                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§APIå‘¼ã³å‡ºã—
                    response = await asyncio.wait_for(
                        self.model.generate_content_async(
                            prompt,
                            generation_config=generation_config,
                            stream=True
                        ),
                        timeout=timeout_seconds
                    )

                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
                    chunk_count = 0
                    total_chars = 0
                    async for chunk in response:
                        if chunk.text:
                            yield chunk.text
                            chunk_count += 1
                            total_chars += len(chunk.text)

                    logger.info(f"âœ… Gemini streaming completed - Chunks: {chunk_count}, Total chars: {total_chars}")

                else:
                    logger.info("ğŸ“¡ Calling Gemini API (non-streaming)...")

                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§APIå‘¼ã³å‡ºã—
                    response = await asyncio.wait_for(
                        self.model.generate_content_async(
                            prompt,
                            generation_config=generation_config
                        ),
                        timeout=timeout_seconds
                    )

                    if response.text:
                        yield response.text
                        logger.info(f"âœ… Gemini response received - Length: {len(response.text)} chars")
                    else:
                        logger.warning("âš ï¸ Gemini returned empty response")
                        yield "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

            except asyncio.TimeoutError:
                logger.error(f"âŒ Vertex AI API timeout after {timeout_seconds}s")
                yield f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™ï¼ˆ{timeout_seconds}ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

        except Exception as e:
            logger.error(f"âŒ Response generation failed: {e}", exc_info=True)
            error_message = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"Returning error message to client: {error_message}")
            yield error_message

    async def _generate_with_thinking(
        self,
        prompt: str,
        stream: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        æ€è€ƒãƒ¢ãƒ¼ãƒ‰ã§å¿œç­”ã‚’ç”Ÿæˆ

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–

        Yields:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯
        """
        try:
            logger.info("ğŸ“¡ Calling Gemini API with Thinking Mode...")

            # ThinkingConfigã‚’æ§‹ç¯‰
            thinking_config = None
            if settings.vertex_ai_thinking_budget != 0:  # 0ä»¥å¤–ã®å ´åˆã¯æ€è€ƒãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹
                thinking_config = types.ThinkingConfig(
                    thinking_budget=settings.vertex_ai_thinking_budget,
                    include_thoughts=settings.vertex_ai_include_thoughts
                )

            # GenerateContentConfigã‚’æ§‹ç¯‰
            config = types.GenerateContentConfig(
                temperature=settings.vertex_ai_temperature,
                max_output_tokens=settings.vertex_ai_max_output_tokens,
                top_p=settings.vertex_ai_top_p,
                top_k=settings.vertex_ai_top_k,
                thinking_config=thinking_config
            )

            # â˜…â˜…â˜… Google Gen AI APIå‘¼ã³å‡ºã—: 1å›ã®ã¿å®Ÿè¡Œ â˜…â˜…â˜…
            if stream:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                def _stream_generate():
                    return self.genai_client.models.generate_content_stream(
                        model=settings.vertex_ai_generation_model,
                        contents=prompt,
                        config=config
                    )

                # åŒæœŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’éåŒæœŸã«å¤‰æ›
                chunk_count = 0
                total_chars = 0

                # asyncio.to_threadã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’éåŒæœŸåŒ–
                loop = asyncio.get_event_loop()
                generator = await loop.run_in_executor(None, _stream_generate)

                for chunk in generator:
                    if hasattr(chunk, 'text') and chunk.text:
                        yield chunk.text
                        chunk_count += 1
                        total_chars += len(chunk.text)

                logger.info(f"âœ… Gemini streaming (Thinking Mode) completed - Chunks: {chunk_count}, Total chars: {total_chars}")

            else:
                # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                def _generate():
                    return self.genai_client.models.generate_content(
                        model=settings.vertex_ai_generation_model,
                        contents=prompt,
                        config=config
                    )

                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(None, _generate)

                if hasattr(response, 'text') and response.text:
                    yield response.text
                    logger.info(f"âœ… Gemini response (Thinking Mode) received - Length: {len(response.text)} chars")
                else:
                    logger.warning("âš ï¸ Gemini (Thinking Mode) returned empty response")
                    yield "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

        except Exception as e:
            logger.error(f"âŒ Thinking Mode generation failed: {e}", exc_info=True)
            error_message = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æ€è€ƒãƒ¢ãƒ¼ãƒ‰ã§ã®å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"Returning error message to client: {error_message}")
            yield error_message

    def _build_prompt_with_history(
        self,
        query: str,
        context: List[Dict[str, Any]],
        history: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        ä¼šè©±å±¥æ­´ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            query: ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            context: RAGæ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            history: ä¼šè©±å±¥æ­´ï¼ˆæœ€æ–°10ä»¶ç¨‹åº¦ï¼‰

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        context_str = ""
        for i, item in enumerate(context, 1):
            title = item.get('title', '')
            content = item.get('content', '')
            source = item.get('source_type', '')
            date = item.get('date', '')

            context_str += f"\nã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ {i}ã€‘\n"
            if title:
                context_str += f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
            if source:
                context_str += f"ã‚½ãƒ¼ã‚¹: {source}\n"
            if date:
                context_str += f"æ—¥ä»˜: {date}\n"
            context_str += f"å†…å®¹:\n{content}\n"

        # ä¼šè©±å±¥æ­´æ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        history_str = ""
        if history and len(history) > 0:
            history_str = "\n# ä¼šè©±å±¥æ­´\n"
            for msg in history[-10:]:  # æœ€æ–°10ä»¶ã®ã¿
                role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg.get('role') == 'user' else "AI"
                content = msg.get('content', '')
                history_str += f"{role}: {content}\n"
            history_str += "\n"

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        prompt = f"""ã‚ãªãŸã¯åŒ»ç™‚ãƒ»çœ‹è­·è¨˜éŒ²ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ä¼šè©±å±¥æ­´ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚
{history_str}
# æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{context_str}

# ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{query}

# å›ç­”ã®è¦ä»¶
- **Markdownå½¢å¼**ã§å›ç­”ã‚’æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ï¼ˆè¦‹å‡ºã—ã€ãƒªã‚¹ãƒˆã€å¤ªå­—ãªã©ã‚’æ´»ç”¨ï¼‰
- æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„
- **ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã€æ–‡è„ˆã‚’ç†è§£ã—ãŸå›ç­”ã‚’ã—ã¦ãã ã•ã„**
- ã€Œå½¼ã€ã€Œãã‚Œã€ã€Œãã®ä»¶ã€ãªã©ã®ä»£åè©ã¯ã€å±¥æ­´ã‹ã‚‰æ–‡è„ˆã‚’æ¨æ¸¬ã—ã¦ãã ã•ã„
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„
- åŒ»ç™‚ç”¨èªã¯åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å„ªå…ˆçš„ã«æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
- æ—¥ä»˜ã‚„æ™‚ç³»åˆ—ãŒé‡è¦ãªå ´åˆã¯ã€æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
- å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å‡ºå…¸ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€ã‚½ãƒ¼ã‚¹ã€æ—¥ä»˜ï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„

# å›ç­”"""

        return prompt

    def _build_prompt(
        self,
        query: str,
        context: List[Dict[str, Any]]
    ) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            context: æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        context_str = ""
        for i, item in enumerate(context, 1):
            title = item.get('title', '')
            content = item.get('content', '')
            source = item.get('source_type', '')
            date = item.get('date', '')

            context_str += f"\nã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ {i}ã€‘\n"
            if title:
                context_str += f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
            if source:
                context_str += f"ã‚½ãƒ¼ã‚¹: {source}\n"
            if date:
                context_str += f"æ—¥ä»˜: {date}\n"
            context_str += f"å†…å®¹:\n{content}\n"

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        prompt = f"""ã‚ãªãŸã¯åŒ»ç™‚ãƒ»çœ‹è­·è¨˜éŒ²ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

# æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{context_str}

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{query}

# å›ç­”ã®è¦ä»¶
- **Markdownå½¢å¼**ã§å›ç­”ã‚’æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ï¼ˆè¦‹å‡ºã—ã€ãƒªã‚¹ãƒˆã€å¤ªå­—ãªã©ã‚’æ´»ç”¨ï¼‰
- æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„
- åŒ»ç™‚ç”¨èªã¯åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å„ªå…ˆçš„ã«æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
- æ—¥ä»˜ã‚„æ™‚ç³»åˆ—ãŒé‡è¦ãªå ´åˆã¯ã€æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
- å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å‡ºå…¸ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€ã‚½ãƒ¼ã‚¹ã€æ—¥ä»˜ï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„

# å›ç­”"""

        return prompt

    async def generate_summary(
        self,
        text: str,
        max_length: int = 200
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ã‚’ç”Ÿæˆ

        Args:
            text: è¦ç´„ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            max_length: æœ€å¤§æ–‡å­—æ•°

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_length}æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# ãƒ†ã‚­ã‚¹ãƒˆ
{text}

# è¦ç´„"""

            generation_config = {
                'temperature': 0.3,
                'max_output_tokens': max_length * 2,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯æ–‡å­—æ•°ã®ç´„2å€
                'top_p': 0.95,
                'top_k': 40
            }

            response = await self.model.generate_content_async(
                prompt,
                generation_config=generation_config
            )

            return response.text.strip()

        except Exception as e:
            logger.error(f"Summary generation failed: {e}", exc_info=True)
            return text[:max_length] + "..."


# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
_gemini_service: Optional[GeminiService] = None


def get_gemini_service() -> GeminiService:
    """
    Geminiã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰

    Returns:
        GeminiService: Geminiã‚µãƒ¼ãƒ“ã‚¹
    """
    global _gemini_service
    if _gemini_service is None:
        _gemini_service = GeminiService()
    return _gemini_service
