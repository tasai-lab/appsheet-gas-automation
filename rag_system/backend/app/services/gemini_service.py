"""
Geminiç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹

Vertex AI Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

import logging
from typing import List, Dict, Any, AsyncGenerator, Optional

import vertexai
from vertexai.generative_models import GenerativeModel, Content, Part
from google.auth import default

from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class GeminiService:
    """Geminiç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        # Google Cloudèªè¨¼
        credentials, project = default()

        # Vertex AIåˆæœŸåŒ–
        vertexai.init(
            project=settings.gcp_project_id,
            location=settings.gcp_location,
            credentials=credentials
        )

        # Gemini ãƒ¢ãƒ‡ãƒ«
        self.model = GenerativeModel(settings.vertex_ai_generation_model)

        logger.info(
            f"Gemini Service initialized - "
            f"Model: {settings.vertex_ai_generation_model}"
        )

    async def generate_response(
        self,
        query: str,
        context: List[Dict[str, Any]],
        stream: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        å¿œç­”ã‚’ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            context: æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆKnowledgeItemsã®ãƒªã‚¹ãƒˆï¼‰
            stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–

        Yields:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            prompt = self._build_prompt(query, context)

            logger.info(f"ğŸš€ Starting Gemini generation - Query: {query[:50]}..., Context: {len(context)} items, Stream: {stream}")

            # ç”Ÿæˆè¨­å®š
            generation_config = {
                'temperature': settings.vertex_ai_temperature,
                'max_output_tokens': settings.vertex_ai_max_output_tokens,
                'top_p': settings.vertex_ai_top_p,
                'top_k': settings.vertex_ai_top_k
            }

            # âš ï¸ TEMPORARY: Vertex AI APIãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹ãŸã‚ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
            logger.warning("âš ï¸ Using context-based response (Vertex AI API timeout issue)")

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸºã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ§‹ç¯‰
            if context:
                mock_response = f"# {query}\n\n"
                mock_response += f"æ¤œç´¢ã«ã‚ˆã‚Š{len(context)}ä»¶ã®é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n"

                for i, item in enumerate(context, 1):
                    title = item.get('title', 'æƒ…å ±ãªã—')
                    content = item.get('content', '')
                    source = item.get('source_type', '')
                    date = item.get('date', '')

                    mock_response += f"## {i}. {title}\n\n"

                    if date:
                        mock_response += f"**æ—¥ä»˜**: {date}\n\n"
                    if source:
                        mock_response += f"**ã‚½ãƒ¼ã‚¹**: {source}\n\n"

                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é©åˆ‡ãªé•·ã•ã«åˆ¶é™
                    if len(content) > 500:
                        content = content[:500] + "..."

                    mock_response += f"{content}\n\n"
                    mock_response += "---\n\n"

                mock_response += "\n\n> âš ï¸ ã“ã‚Œã¯æ¤œç´¢çµæœã®è¡¨ç¤ºã§ã™ã€‚Vertex AI APIã®æ¥ç¶šå•é¡Œã«ã‚ˆã‚Šã€AIç”Ÿæˆå›ç­”ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
            else:
                mock_response = f"# {query}\n\nç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\n> âš ï¸ Vertex AI APIã®æ¥ç¶šå•é¡Œã«ã‚ˆã‚Šã€æ¤œç´¢çµæœã®ã¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚"

            yield mock_response
            logger.info(f"âœ… Context-based response sent - Length: {len(mock_response)} chars, Context items: {len(context)}")

            # å…ƒã®ã‚³ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
            # if stream:
            #     logger.info("ğŸ“¡ Calling Gemini API with streaming...")
            #     response = await self.model.generate_content_async(
            #         prompt,
            #         generation_config=generation_config,
            #         stream=True
            #     )
            #     ...
            # else:
            #     logger.info("ğŸ“¡ Calling Gemini API (non-streaming)...")
            #     response = await self.model.generate_content_async(
            #         prompt,
            #         generation_config=generation_config
            #     )
            #     ...

        except Exception as e:
            logger.error(f"âŒ Response generation failed: {e}", exc_info=True)
            error_message = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"Returning error message to client: {error_message}")
            yield error_message

    def _build_prompt(
        self,
        query: str,
        context: List[Dict[str, Any]]
    ) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            context: æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        context_str = ""
        for i, item in enumerate(context, 1):
            title = item.get('title', '')
            content = item.get('content', '')
            source = item.get('source_type', '')
            date = item.get('date', '')

            context_str += f"\nã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ {i}ã€‘\n"
            if title:
                context_str += f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
            if source:
                context_str += f"ã‚½ãƒ¼ã‚¹: {source}\n"
            if date:
                context_str += f"æ—¥ä»˜: {date}\n"
            context_str += f"å†…å®¹:\n{content}\n"

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        prompt = f"""ã‚ãªãŸã¯åŒ»ç™‚ãƒ»çœ‹è­·è¨˜éŒ²ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

# æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{context_str}

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{query}

# å›ç­”ã®è¦ä»¶
- **Markdownå½¢å¼**ã§å›ç­”ã‚’æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ï¼ˆè¦‹å‡ºã—ã€ãƒªã‚¹ãƒˆã€å¤ªå­—ãªã©ã‚’æ´»ç”¨ï¼‰
- æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„
- åŒ»ç™‚ç”¨èªã¯åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å„ªå…ˆçš„ã«æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
- æ—¥ä»˜ã‚„æ™‚ç³»åˆ—ãŒé‡è¦ãªå ´åˆã¯ã€æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
- å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å‡ºå…¸ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€ã‚½ãƒ¼ã‚¹ã€æ—¥ä»˜ï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„

# å›ç­”"""

        return prompt

    async def generate_summary(
        self,
        text: str,
        max_length: int = 200
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ã‚’ç”Ÿæˆ

        Args:
            text: è¦ç´„ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            max_length: æœ€å¤§æ–‡å­—æ•°

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_length}æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# ãƒ†ã‚­ã‚¹ãƒˆ
{text}

# è¦ç´„"""

            generation_config = {
                'temperature': 0.3,
                'max_output_tokens': max_length * 2,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯æ–‡å­—æ•°ã®ç´„2å€
                'top_p': 0.95,
                'top_k': 40
            }

            response = await self.model.generate_content_async(
                prompt,
                generation_config=generation_config
            )

            return response.text.strip()

        except Exception as e:
            logger.error(f"Summary generation failed: {e}", exc_info=True)
            return text[:max_length] + "..."


# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
_gemini_service: Optional[GeminiService] = None


def get_gemini_service() -> GeminiService:
    """
    Geminiã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰

    Returns:
        GeminiService: Geminiã‚µãƒ¼ãƒ“ã‚¹
    """
    global _gemini_service
    if _gemini_service is None:
        _gemini_service = GeminiService()
    return _gemini_service
