"""
ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""

import logging
import time
import uuid
from datetime import datetime

from fastapi import APIRouter, HTTPException, status
from sse_starlette.sse import EventSourceResponse

from app.config import get_settings
from app.models.request import ChatRequest
from app.models.response import ChatMessage, ChatResponse, StreamChunk

router = APIRouter()
logger = logging.getLogger(__name__)
settings = get_settings()


@router.post(
    "/stream",
    response_class=EventSourceResponse,
    status_code=status.HTTP_200_OK,
    summary="ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ",
    description="SSEã«ã‚ˆã‚‹ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆå¿œç­”"
)
async def chat_stream(request: ChatRequest):
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ

    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ

    Returns:
        EventSourceResponse: SSEã‚¹ãƒˆãƒªãƒ¼ãƒ 

    Raises:
        HTTPException: ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼æ™‚
    """
    start_time = time.time()

    try:
        logger.info(f"Chat message: {request.message}")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ
        session_id = request.session_id or str(uuid.uuid4())

        async def event_generator():
            """SSEã‚¤ãƒ™ãƒ³ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
            try:
                # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
                from app.services.rag_engine import get_hybrid_search_engine
                from app.services.gemini_service import get_gemini_service
                from app.models.response import KnowledgeItem

                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: æ¤œç´¢é–‹å§‹
                search_start_time = time.time()
                yield {
                    "event": "message",
                    "data": StreamChunk(
                        type="status",
                        status="searching",
                        metadata={"message": "æƒ…å ±ã‚’æ¤œç´¢ä¸­..."}
                    ).model_dump_json()
                }

                engine = get_hybrid_search_engine()
                gemini_service = get_gemini_service()

                # Hybrid Searchå®Ÿè¡Œ
                search_result = engine.search(
                    query=request.message,
                    domain=request.domain,
                    client_id=request.client_id,
                    top_k=request.context_size or 5
                )

                search_time = (time.time() - search_start_time) * 1000

                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Œäº†
                yield {
                    "event": "message",
                    "data": StreamChunk(
                        type="status",
                        status="reranking",
                        metadata={
                            "message": f"çµæœã‚’æœ€é©åŒ–ã—ã¾ã—ãŸ ({len(search_result.get('results', []))}ä»¶)",
                            "search_time_ms": search_time
                        }
                    ).model_dump_json()
                }

                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’StreamChunkã¨ã—ã¦é€ä¿¡
                context_items = [
                    KnowledgeItem(
                        id=result.get('id', ''),
                        domain=result.get('domain', ''),
                        title=result.get('title', ''),
                        content=result.get('content', ''),
                        score=result.get('rank_score', 0.0),
                        source_type=result.get('source_type'),
                        source_table=result.get('source_table'),
                        source_id=result.get('source_id'),
                        tags=result.get('tags', '').split(',') if result.get('tags') else []
                    )
                    for result in search_result.get('results', [])
                ]

                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡
                yield {
                    "event": "message",
                    "data": StreamChunk(
                        type="context",
                        context=context_items
                    ).model_dump_json()
                }

                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: ç”Ÿæˆé–‹å§‹
                generation_start_time = time.time()
                yield {
                    "event": "message",
                    "data": StreamChunk(
                        type="status",
                        status="generating",
                        metadata={"message": "å›ç­”ã‚’ç”Ÿæˆä¸­..."}
                    ).model_dump_json()
                }

                # 2. Gemini APIå‘¼ã³å‡ºã— (éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ - ãƒ‡ãƒãƒƒã‚°ç”¨)
                logger.info("ğŸ”µ Starting Gemini API call for response generation (non-streaming)...")

                full_response = ""
                async for text_chunk in gemini_service.generate_response(
                    query=request.message,
                    context=search_result.get('results', []),
                    stream=False  # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã«å¤‰æ›´
                ):
                    full_response += text_chunk

                logger.info(f"âœ… Gemini response received - Length: {len(full_response)} chars")

                # ä¸€åº¦ã«å…¨æ–‡ã‚’é€ä¿¡
                if full_response:
                    yield {
                        "event": "message",
                        "data": StreamChunk(
                            type="text",
                            content=full_response
                        ).model_dump_json()
                    }

                generation_time = (time.time() - generation_start_time) * 1000
                total_time = (time.time() - start_time) * 1000

                # 3. å®Œäº†é€šçŸ¥
                logger.info(f"ğŸ“Š Sending completion event - Total: {total_time:.2f}ms, Search: {search_time:.2f}ms, Generation: {generation_time:.2f}ms")
                yield {
                    "event": "message",
                    "data": StreamChunk(
                        type="done",
                        suggested_terms=search_result.get('suggested_terms', []),
                        metadata={
                            "total_time_ms": total_time,
                            "search_time_ms": search_time,
                            "generation_time_ms": generation_time
                        }
                    ).model_dump_json()
                }

            except Exception as e:
                logger.error(f"Stream error: {e}", exc_info=True)
                yield {
                    "event": "error",
                    "data": StreamChunk(
                        type="error",
                        error=str(e)
                    ).model_dump_json()
                }

        return EventSourceResponse(event_generator())

    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


@router.post(
    "",
    response_model=ChatResponse,
    status_code=status.HTTP_200_OK,
    summary="ãƒãƒ£ãƒƒãƒˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰",
    description="é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"
)
async def chat(request: ChatRequest):
    """
    ãƒãƒ£ãƒƒãƒˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ

    Returns:
        ChatResponse: ãƒãƒ£ãƒƒãƒˆå¿œç­”

    Raises:
        HTTPException: ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼æ™‚
    """
    start_time = time.time()

    try:
        logger.info(f"Chat message (non-streaming): {request.message}")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ
        session_id = request.session_id or str(uuid.uuid4())

        # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
        from app.services.rag_engine import get_hybrid_search_engine
        from app.services.gemini_service import get_gemini_service
        from app.models.response import KnowledgeItem

        engine = get_hybrid_search_engine()
        gemini_service = get_gemini_service()

        # Hybrid Searchå®Ÿè¡Œ
        search_result = engine.search(
            query=request.message,
            domain=request.domain,
            client_id=request.client_id,
            top_k=request.context_size or 5
        )

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚’æ§‹ç¯‰
        context_items = [
            KnowledgeItem(
                id=result.get('id', ''),
                domain=result.get('domain', ''),
                title=result.get('title', ''),
                content=result.get('content', ''),
                score=result.get('rank_score', 0.0),
                source_type=result.get('source_type'),
                source_table=result.get('source_table'),
                source_id=result.get('source_id'),
                tags=result.get('tags', '').split(',') if result.get('tags') else []
            )
            for result in search_result.get('results', [])
        ]

        # 2. Geminiå¿œç­”ç”Ÿæˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        full_response = ""
        async for text_chunk in gemini_service.generate_response(
            query=request.message,
            context=search_result.get('results', []),
            stream=False
        ):
            full_response += text_chunk

        # 3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
        return ChatResponse(
            session_id=session_id,
            message=ChatMessage(
                role="assistant",
                content=full_response,
                timestamp=datetime.utcnow()
            ),
            context=context_items,
            suggested_terms=search_result.get('suggested_terms', []),
            processing_time_ms=(time.time() - start_time) * 1000
        )

    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )
