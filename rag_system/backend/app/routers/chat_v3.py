"""
ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ V3

RAG Engine V3ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿãƒ»é«˜ç²¾åº¦ãªãƒãƒ£ãƒƒãƒˆAPIã€‚
é€²æ—ã‚¤ãƒ™ãƒ³ãƒˆä»˜ãSSEã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã€‚
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import AsyncGenerator, Dict, Any

from fastapi import APIRouter, Depends, HTTPException, status
from sse_starlette.sse import EventSourceResponse

from app.config import get_settings
from app.middleware.auth import verify_firebase_token
from app.models.request import ChatRequest
from app.models.response import StreamChunk
from app.services.firestore_chat_history import get_firestore_chat_history_service
from app.services.gemini_service import get_gemini_service
from app.services.rag_engine_v3 import get_rag_engine_v3

router = APIRouter()
logger = logging.getLogger(__name__)
settings = get_settings()


@router.post(
    "/stream/v3",
    status_code=status.HTTP_200_OK,
    summary="ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ V3",
    description="RAG Engine V3ã‚’ä½¿ç”¨ã—ãŸSSEã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆï¼ˆé€²æ—ã‚¤ãƒ™ãƒ³ãƒˆä»˜ãï¼‰",
)
async def chat_stream_v3(
    request: ChatRequest, user: dict = Depends(verify_firebase_token)
):
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ V3

    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        user: èªè¨¼æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±

    Returns:
        EventSourceResponse: SSEã‚¹ãƒˆãƒªãƒ¼ãƒ 

    Pipeline:
        1. optimizing - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ä¸­ï¼ˆ10%ï¼‰
        2. searching - æƒ…å ±ã‚’æ¤œç´¢ä¸­ï¼ˆ30%ï¼‰
        3. reranking - çµæœã‚’æœ€é©åŒ–ä¸­ï¼ˆ60%ï¼‰
        4. generating - å›ç­”ã‚’ç”Ÿæˆä¸­ï¼ˆ80%ï¼‰
        5. done - å®Œäº†ï¼ˆ100%ï¼‰
    """
    start_time = time.time()

    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“¨ Chat V3 Request: {request.message[:100]}...")
        logger.info(f"   Session: {request.session_id}")
        logger.info(f"   Client: {request.client_id}")
        logger.info("=" * 80)

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ
        session_id = request.session_id or str(uuid.uuid4())

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDå–å¾—
        user_uid = user.get("uid") if user else "anonymous"

        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿å­˜ï¼ˆFirestoreï¼‰
        if settings.use_firestore_chat_history:
            try:
                firestore_history = get_firestore_chat_history_service()
                firestore_history.save_user_message(
                    session_id=session_id,
                    user_id=user_uid,
                    message=request.message,
                    timestamp=datetime.now(),
                )
                logger.info(f"âœ… User message saved - Session: {session_id}")
            except Exception as e:
                logger.error(f"âš ï¸ Failed to save user message: {e}", exc_info=True)

        async def event_generator() -> AsyncGenerator[Dict[str, Any], None]:
            """SSEã‚¤ãƒ™ãƒ³ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆV3ï¼‰"""
            accumulated_response = ""
            context_ids = []

            try:
                # ================================================================
                # Stage 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ï¼ˆ10%ï¼‰
                # ================================================================
                yield {
                    "event": "message",
                    "data": json.dumps(
                        StreamChunk(
                            type="progress",
                            status="optimizing",
                            progress=10,
                            metadata={"message": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æœ€é©åŒ–ä¸­..."},
                        ).model_dump()
                    ),
                }
                await asyncio.sleep(0)  # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™

                # ================================================================
                # Stage 2: RAG Engine V3 æ¤œç´¢ï¼ˆ30% â†’ 60%ï¼‰
                # ================================================================
                yield {
                    "event": "message",
                    "data": json.dumps(
                        StreamChunk(
                            type="progress",
                            status="searching",
                            progress=30,
                            metadata={"message": "æƒ…å ±ã‚’æ¤œç´¢ä¸­..."},
                        ).model_dump()
                    ),
                }
                await asyncio.sleep(0)

                # RAG Engine V3ã§æ¤œç´¢
                rag_engine = get_rag_engine_v3()

                search_result = await rag_engine.search(
                    query=request.message,
                    client_id=request.client_id,
                    client_name=None,  # TODO: client_nameã‚’å–å¾—
                    domain=request.domain,
                    top_k=20,  # V3ã¯20ä»¶è¿”ã™
                )

                # æ¤œç´¢çµæœ
                context = search_result["results"]
                optimized_query = search_result["optimized_query"]
                metrics = search_result["metrics"]

                logger.info(f"âœ… RAG Engine V3 Search completed: {len(context)} results")
                logger.info(f"   Optimized Query: {optimized_query[:100]}...")
                logger.info(f"   Duration: {metrics['total_duration']:.3f}ç§’")

                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆIDã‚’ä¿å­˜
                context_ids = [item.get("id") for item in context if item.get("id")]

                # ================================================================
                # Stage 3: ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Œäº†ï¼ˆ60%ï¼‰
                # ================================================================
                yield {
                    "event": "message",
                    "data": json.dumps(
                        StreamChunk(
                            type="progress",
                            status="reranking",
                            progress=60,
                            metadata={
                                "message": "çµæœã‚’æœ€é©åŒ–ä¸­...",
                                "search_duration": metrics["total_duration"],
                            },
                        ).model_dump()
                    ),
                }
                await asyncio.sleep(0)

                # ================================================================
                # Stage 4: å›ç­”ç”Ÿæˆï¼ˆ80% â†’ 100%ï¼‰
                # ================================================================
                yield {
                    "event": "message",
                    "data": json.dumps(
                        StreamChunk(
                            type="progress",
                            status="generating",
                            progress=80,
                            metadata={"message": "å›ç­”ã‚’ç”Ÿæˆä¸­..."},
                        ).model_dump()
                    ),
                }
                await asyncio.sleep(0)

                # ä¼šè©±å±¥æ­´å–å¾—ï¼ˆæœ€æ–°10ä»¶ï¼‰
                history = []
                if settings.use_firestore_chat_history:
                    try:
                        firestore_history = get_firestore_chat_history_service()
                        history = firestore_history.get_recent_messages(
                            session_id=session_id, limit=10
                        )
                    except Exception as e:
                        logger.error(f"âš ï¸ Failed to fetch history: {e}")

                # Gemini Service ã§å›ç­”ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                gemini_service = get_gemini_service()

                async for chunk in gemini_service.generate_response(
                    query=optimized_query,  # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                    context=context,
                    history=history,
                    stream=True,
                ):
                    accumulated_response += chunk

                    # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’é€ä¿¡
                    yield {
                        "event": "message",
                        "data": json.dumps(
                            StreamChunk(
                                type="content", content=chunk, metadata={}
                            ).model_dump()
                        ),
                    }
                    await asyncio.sleep(0)

                # ================================================================
                # Stage 5: å®Œäº†ï¼ˆ100%ï¼‰
                # ================================================================
                total_duration = time.time() - start_time

                logger.info("âœ… Chat V3 completed")
                logger.info(f"   Response length: {len(accumulated_response)} chars")
                logger.info(f"   Total duration: {total_duration:.3f}ç§’")

                # å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
                yield {
                    "event": "message",
                    "data": json.dumps(
                        StreamChunk(
                            type="done",
                            status="completed",
                            progress=100,
                            metadata={
                                "total_duration": total_duration,
                                "search_duration": metrics["total_duration"],
                                "context_count": len(context),
                                "response_length": len(accumulated_response),
                            },
                        ).model_dump()
                    ),
                }

                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿å­˜ï¼ˆFirestoreï¼‰
                if settings.use_firestore_chat_history:
                    try:
                        firestore_history = get_firestore_chat_history_service()
                        firestore_history.save_assistant_message(
                            session_id=session_id,
                            message=accumulated_response,
                            context_ids=context_ids,
                            timestamp=datetime.now(),
                        )
                        logger.info(f"âœ… Assistant message saved - Session: {session_id}")
                    except Exception as e:
                        logger.error(
                            f"âš ï¸ Failed to save assistant message: {e}", exc_info=True
                        )

            except Exception as e:
                logger.error(f"âŒ Chat V3 generation failed: {e}", exc_info=True)

                # ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
                yield {
                    "event": "message",
                    "data": json.dumps(
                        StreamChunk(
                            type="error",
                            status="error",
                            metadata={"error": str(e), "error_type": type(e).__name__},
                        ).model_dump()
                    ),
                }

        # EventSourceResponse ã‚’è¿”ã™ï¼ˆSSEã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        return EventSourceResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # nginxãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
            },
        )

    except Exception as e:
        logger.error(f"âŒ Chat V3 request failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": str(e), "error_type": type(e).__name__},
        )
